url,title,content,scraped_at
https://en.wikipedia.org/wiki/Artificial_intelligence,Artificial intelligence - Wikipedia,"Artificial intelligence Contents Goals Techniques Applications Ethics History Philosophy Future In fiction See also Explanatory notes References Further reading External links Reasoning and problem-solving Knowledge representation Planning and decision-making Learning Natural language processing Perception Social intelligence General intelligence Search and optimization Logic Probabilistic methods for uncertain reasoning Classifiers and statistical learning methods Artificial neural networks Deep learning GPT Hardware and software Health and medicine Games Mathematics Finance Military Generative AI Agents Sexuality Other industry-specific tasks Risks and harm Ethical machines and alignment Open source Frameworks Regulation Defining artificial intelligence Evaluating approaches to AI Machine consciousness, sentience, and mind Superintelligence and the singularity Transhumanism Decomputing AI textbooks History of AI Other sources Artificial intelligenceAI refers to the capability ofcomputational systemsto perform tasks typically associated withhuman intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is afield of researchincomputer sciencethat develops and studies methods andsoftwarethat enable machines toperceive their environmentand uselearningandintelligenceto take actions that maximize their chances of achieving defined goals.1Such machines may be called AIs. High-profileapplications of AIinclude advancedweb search enginese.g.,Google Search;recommendation systemsused byYouTube,Amazon, andNetflix;virtual assistantse.g.,Google Assistant,Siri, andAlexa;autonomous vehiclese.g.,Waymo;generativeandcreativetools e.g.,ChatGPTandAI art; andsuperhumanplay and analysis instrategy gamese.g.,chessandGo. However, many AI applications are not perceived as AI: ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it'snot labeled AI anymore.""23 Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning,reasoning,knowledge representation,planning,natural language processing,perception, and support forrobotics.aGeneral intelligencethe ability to complete any task performed by a human on an at least equal levelis among the field's long-term goals.4To reach these goals, AI researchers have adapted and integrated a wide range of techniques, includingsearchandmathematical optimization,formal logic,artificial neural networks, and methods based onstatistics,operations research, andeconomics.bAI also draws uponpsychology,linguistics,philosophy,neuroscience, and other fields.5 Artificial intelligence was founded as an academic discipline in 1956,6and the field went through multiple cycles of optimism throughoutits history,78followed by periods of disappointment and loss of funding, known asAI winters.910Funding and interest vastly increased after 2012 whendeep learningoutperformed previous AI techniques.11This growth accelerated further after 2017 with thetransformer architecture,12and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoingprogressin what has become known as theAI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about therisks of AIandits long-term effectsin the future, prompting discussions aboutregulatory policiesto ensure thesafety and benefits of the technology. The general problem of simulating or creating intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.a Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logicaldeductions.13By the late 1980s and 1990s, methods were developed for dealing withuncertainor incomplete information, employing concepts fromprobabilityandeconomics.14 Many of these algorithms are insufficient for solving large reasoning problems because they experience a ""combinatorial explosion"": They become exponentially slower as the problems grow.15Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.16Accurate and efficient reasoning is an unsolved problem. Knowledge representationandknowledge engineering17allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,18scene interpretation,19clinical decision support,20knowledge discovery mining ""interesting"" and actionable inferences from largedatabases,21and other areas.22 Aknowledge baseis a body of knowledge represented in a form that can be used by a program. Anontologyis the set of objects, relations, concepts, and properties used by a particular domain of knowledge.23Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;24situations, events, states, and time;25causes and effects;26knowledge about knowledge what we know about what other people know;27default reasoningthings that humans assume are true until they are told differently and will remain true even when other facts are changing;28and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge the set of atomic facts that the average person knows is enormous;29and the sub-symbolic form of most commonsense knowledge much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally.16There is also the difficulty ofknowledge acquisition, the problem of obtaining knowledge for AI applications.c An ""agent"" is anything that perceives and takes actions in the world. Arational agenthas goals or preferences and takes actions to make them happen.d32Inautomated planning, the agent has a specific goal.33Inautomated decision-making, the agent has preferencesthere are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation called the ""utility"" that measures how much the agent prefers it. For each possible action, it can calculate the ""expected utility"": theutilityof all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.34 Inclassical planning, the agent knows exactly what the effect of any action will be.35In most real-world problems, however, the agent may not be certain about the situation they are in it is ""unknown"" or ""unobservable"" and it may not know for certain what will happen after each possible action it is not ""deterministic"". It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.36 In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned e.g., withinverse reinforcement learning, or the agent can seek information to improve its preferences.37Information value theorycan be used to weigh the value of exploratory or experimental actions.38The space of possible future actions and situations is typicallyintractablylarge, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. AMarkov decision processhas atransition modelthat describes the probability that a particular action will change the state in a particular way and areward functionthat supplies the utility of each state and the cost of each action. Apolicyassociates a decision with each possible state. The policy could be calculated e.g., byiteration, beheuristic, or it can be learned.39 Game theorydescribes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.40 Machine learningis the study of programs that can improve their performance on a given task automatically.41It has been a part of AI from the beginning.e There are several kinds of machine learning.Unsupervised learninganalyzes a stream of data and finds patterns and makes predictions without any other guidance.44Supervised learningrequires labeling the training data with the expected answers, and comes in two main varieties:classificationwhere the program must learn to predict what category the input belongs in andregressionwhere the program must deduce a numeric function based on numeric input.45 Inreinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as ""good"".46Transfer learningis when the knowledge gained from one problem is applied to a new problem.47Deep learningis a type of machine learning that runs inputs through biologically inspiredartificial neural networksfor all of these types of learning.48 Computational learning theorycan assess learners bycomputational complexity, bysample complexityhow much data is required, or by other notions ofoptimization.49 Natural language processingNLP50allows programs to read, write and communicate in human languages such asEnglish. Specific problems includespeech recognition,speech synthesis,machine translation,information extraction,information retrievalandquestion answering.51 Early work, based onNoam Chomsky'sgenerative grammarandsemantic networks, had difficulty withword-sense disambiguationfunless restricted to small domains called ""micro-worlds"" due to the common sense knowledge problem29.Margaret Mastermanbelieved that it was meaning and not grammar that was the key to understanding languages, and thatthesauriand not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP includeword embeddingrepresenting words, typically asvectorsencoding their meaning,52transformersa deep learning architecture using anattentionmechanism,53and others.54In 2019,generative pre-trained transformeror ""GPT"" language models began to generate coherent text,5556and by 2023, these models were able to get human-level scores on thebar exam,SATtest,GREtest, and many other real-world applications.57 Machine perceptionis the ability to use input from sensors such as cameras, microphones, wireless signals, activelidar, sonar, radar, andtactile sensors to deduce aspects of the world.Computer visionis the ability to analyze visual input.58 The field includesspeech recognition,59image classification,60facial recognition,object recognition,61object tracking,62androbotic perception.63 Affective computingis a field that comprises systems that recognize, interpret, process, or simulate humanfeeling, emotion, and mood.65For example, somevirtual assistantsare programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitatehumancomputer interaction. However, this tends to give na√Øve users an unrealistic conception of the intelligence of existing computer agents.66Moderate successes related to affective computing include textualsentiment analysisand, more recently,multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.67 A machine withartificial general intelligenceshould be able to solve a wide variety of problems with breadth and versatility similar tohuman intelligence.4 AI research uses a wide variety of techniques to accomplish the goals above.b AI can solve many problems by intelligently searching through many possible solutions.68There are two very different kinds of search used in AI:state space searchandlocal search. State space searchsearches through a tree of possible states to try to find a goal state.69For example,planningalgorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process calledmeans-ends analysis.70 Simple exhaustive searches71are rarely sufficient for most real-world problems: thesearch spacethe number of places to search quickly grows toastronomical numbers. The result is a search that istoo slowor never completes.15""Heuristics"" or ""rules of thumb"" can help prioritize choices that are more likely to reach a goal.72 Adversarial searchis used forgame-playingprograms, such as chess or Go. It searches through atreeof possible moves and countermoves, looking for a winning position.73 Local searchusesmathematical optimizationto find a solution to a problem. It begins with some form of guess and refines it incrementally.74 Gradient descentis a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize aloss function. Variants of gradient descent are commonly used to trainneural networks,75through thebackpropagationalgorithm. Another type of local search isevolutionary computation, which aims to iteratively improve a set of candidate solutions by ""mutating"" and ""recombining"" them,selectingonly the fittest to survive each generation.76 Distributed search processes can coordinate viaswarm intelligencealgorithms. Two popular swarm algorithms used in search areparticle swarm optimizationinspired by birdflocking andant colony optimizationinspired byant trails.77 Formallogicis used forreasoningandknowledge representation.78Formal logic comes in two main forms:propositional logicwhich operates on statements that are true or false and useslogical connectivessuch as ""and"", ""or"", ""not"" and ""implies""79andpredicate logicwhich also operates on objects, predicates and relations and usesquantifierssuch as ""EveryXis aY"" and ""There aresomeXs that areYs"".80 Deductive reasoningin logic is the process ofprovinga new statement conclusion from other statements that are given and assumed to be true thepremises.81Proofs can be structured as prooftrees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes byinference rules. Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whoseleaf nodesare labelled by premises oraxioms. In the case ofHorn clauses, problem-solving search can be performed by reasoningforwardsfrom the premises orbackwardsfrom the problem.82In the more general case of the clausal form offirst-order logic,resolutionis a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.83 Inference in both Horn clause logic and first-order logic isundecidable, and thereforeintractable. However, backward reasoning with Horn clauses, which underpins computation in thelogic programminglanguageProlog, isTuring complete. Moreover, its efficiency is competitive with computation in othersymbolic programminglanguages.84 Fuzzy logicassigns a ""degree of truth"" between 0 and 1. It can therefore handle propositions that are vague and partially true.85 Non-monotonic logics, including logic programming withnegation as failure, are designed to handledefault reasoning.28Other specialized versions of logic have been developed to describe many complex domains. Many problems in AI including in reasoning, planning, learning, perception, and robotics require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods fromprobabilitytheory and economics.86Precise mathematical tools have been developed that analyze how an agent can make choices and plan, usingdecision theory,decision analysis,87andinformation value theory.88These tools include models such asMarkov decision processes,89dynamicdecision networks,90game theoryandmechanism design.91 Bayesian networks92are a tool that can be used forreasoningusing theBayesian inferencealgorithm,g94learningusing theexpectationmaximization algorithm,h96planningusingdecision networks97andperceptionusingdynamic Bayesian networks.90 Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time e.g.,hidden Markov modelsorKalman filters.90 The simplest AI applications can be divided into two types: classifiers e.g., ""if shiny then diamond"", on one hand, and controllers e.g., ""if diamond then pick up"", on the other hand.Classifiers98are functions that usepattern matchingto determine the closest match. They can be fine-tuned based on chosen examples usingsupervised learning. Each pattern also called an ""observation"" is labeled with a certain predefined class. All the observations combined with their class labels are known as adata set. When a new observation is received, that observation is classified based on previous experience.45 There are many kinds of classifiers in use.99Thedecision treeis the simplest and most widely used symbolic machine learning algorithm.100K-nearest neighboralgorithm was the most widely used analogical AI until the mid-1990s, andKernel methodssuch as thesupport vector machineSVM displaced k-nearest neighbor in the 1990s.101Thenaive Bayes classifieris reportedly the ""most widely used learner""102at Google, due in part to its scalability.103Neural networksare also used as classifiers.104 An artificial neural network is based on a collection of nodes also known asartificial neurons, which loosely model theneuronsin a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once theweightcrosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.104 Learning algorithms for neural networks uselocal searchto choose the weights that will get the right output for each input during training. The most common training technique is thebackpropagationalgorithm.105Neural networks learn to model complex relationships between inputs and outputs andfind patternsin data. In theory, a neural network can learn any function.106 Infeedforward neural networksthe signal passes in only one direction.107Recurrent neural networksfeed the output signal back into the input, which allows short-term memories of previous input events.Long short term memoryis the most successful network architecture for recurrent networks.108Perceptrons109use only a single layer of neurons; deep learning110uses multiple layers.Convolutional neural networksstrengthen the connection between neurons that are ""close"" to each otherthis is especially important inimage processing, where a local set of neurons mustidentify an ""edge""before the network can identify an object.111 Deep learning110uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, inimage processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.112 Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, includingcomputer vision,speech recognition,natural language processing,image classification,113and others. The reason that deep learning performs so well in so many applications is not known as of 2021.114The sudden success of deep learning in 20122015 did not occur because of some new discovery or theoretical breakthrough deep neural networks and backpropagation had been described by many people, as far back as the 1950sibut because of two factors: the incredible increase in computer power including the hundred-fold increase in speed by switching toGPUs and the availability of vast amounts of training data, especially the giantcurated datasetsused for benchmark testing, such asImageNet.j Generative pre-trained transformersGPT arelarge language modelsLLMs that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a largecorpus of textthat can be from the Internet. The pretraining consists of predicting the nexttokena token being usually a word, subword, or punctuation. Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique calledreinforcement learning from human feedbackRLHF. Current GPT models are prone to generating falsehoods called ""hallucinations"", although this can be reduced with RLHF and quality data. They are used inchatbots, which allow people to ask a question or request a task in simple text.122123 Current models and services includeGeminiformerly Bard,ChatGPT,Grok,Claude,Copilot, andLLaMA.124MultimodalGPT models can process different types of data modalities such as images, videos, sound, and text.125 In the late 2010s,graphics processing unitsGPUs that were increasingly designed with AI-specific enhancements and used with specializedTensorFlowsoftware had replaced previously usedcentral processing unitCPUs as the dominant means for large-scale commercial and academicmachine learningmodels' training.126Specializedprogramming languagessuch asPrologwere used in early AI research,127butgeneral-purpose programming languageslikePythonhave become predominant.128 The transistor density inintegrated circuitshas been observed to roughly double every 18 monthsa trend known asMoore's law, named after theIntelco-founderGordon Moore, who first identified it. Improvements inGPUshave been even faster,129a trend sometimes calledHuang's law,130named afterNvidiaco-founder and CEOJensen Huang. AI and machine learning technology is used in most of the essential applications of the 2020s, including:search enginessuch asGoogle Search,targeting online advertisements,recommendation systemsoffered byNetflix,YouTubeorAmazon, drivinginternet traffic,targeted advertisingAdSense,Facebook,virtual assistantssuch asSiriorAlexa,autonomous vehiclesincludingdrones,ADASandself-driving cars,automatic language translationMicrosoft Translator,Google Translate,facial recognitionApple'sFace IDorMicrosoft'sDeepFaceandGoogle'sFaceNet andimage labelingused byFacebook, Apple'siPhotoandTikTok. The deployment of AI may be overseen by aChief automation officerCAO. The application of AI inmedicineandmedical researchhas the potential to increase patient care and quality of life.131Through the lens of theHippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.132133 For medical research, AI is an important tool for processing and integratingbig data. This is particularly important fororganoidandtissue engineeringdevelopment which usemicroscopyimaging as a key technique in fabrication.134It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.134135New AI tools can deepen the understanding of biomedically relevant pathways. For example,AlphaFold 22021 demonstrated the ability to approximate, in hours rather than months, the 3Dstructure of a protein.136In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.137In 2024, researchers used machine learning to accelerate the search forParkinson's diseasedrug treatments. Their aim was to identify compounds that block the clumping, or aggregation, ofalpha-synucleinthe protein that characterises Parkinson's disease. They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.138139 Game playingprograms have been used since the 1950s to demonstrate and test AI's most advanced techniques.140Deep Bluebecame the first computer chess-playing system to beat a reigning world chess champion,Garry Kasparov, on 11 May 1997.141In 2011, in aJeopardy!quiz showexhibition match,IBM'squestion answering system,Watson, defeated the two greatestJeopardy!champions,Brad RutterandKen Jennings, by a significant margin.142In March 2016,AlphaGowon 4 out of 5 games ofGoin a match with Go championLee Sedol, becoming the firstcomputer Go-playing system to beat a professional Go player withouthandicaps. Then, in 2017, itdefeated Ke Jie, who was the best Go player in the world.143Other programs handleimperfect-informationgames, such as thepoker-playing programPluribus.144DeepMinddeveloped increasingly generalisticreinforcement learningmodels, such as withMuZero, which could be trained to play chess, Go, orAtarigames.145In 2019, DeepMind's AlphaStar achieved grandmaster level inStarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.146In 2021, an AI agent competed in a PlayStationGran Turismocompetition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.147In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseenopen-worldvideo games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.148 Large language models, such asGPT-4,Gemini,Claude,LLaMaorMistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form ofhallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such assupervisedfine-tuning149or trainedclassifierswith human-annotated data to improve answers for new problems and learn from corrections.150A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.151One technique to improve their performance involves training the models to produce correctreasoningsteps, rather than just the correct result.152TheAlibaba Groupdeveloped a version of itsQwenmodels calledQwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84 accuracy on the MATH dataset of competition mathematics problems.153In January 2025, Microsoft proposed the techniquerStar-Maththat leveragesMonte Carlo tree searchand step-by-step reasoning, enabling a relatively small language model likeQwen-7Bto solve 53 of theAIME2024 and 90 of the MATH benchmark problems.154 Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such asAlphaTensor,AlphaGeometryandAlphaProofall fromGoogle DeepMind,155LlemmafromEleutherAI156orJulius.157 When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such asLeanto define mathematical tasks. Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.158 Topological deep learningintegrates varioustopologicalapproaches. Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated ""robot advisers"" have been in use for some years.159 According to Nicolas Firzli, director of theWorld Pensions  Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that ""the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of e.g., sophisticated pension innovation.""160 Various countries are deploying AI military applications.161The main applications enhancecommand and control, communications, sensors, integration and interoperability.162Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous andautonomous vehicles.161AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions,target acquisition, coordination and deconfliction of distributedJoint Firesbetween networked combat vehicles, both human operated andautonomous.162 AI has been used in military operations in Iraq, Syria, Israel and Ukraine.161163164165 Generative artificial intelligenceGenerative AI, GenAI,166or GAI is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.167168169These modelslearnthe underlying patterns and structures of theirtraining dataand use them to produce new data170171based on the input, which often comes in the form of natural languageprompts.172173 Generative AI tools have become more common since an ""AI boom"" in the 2020s. This boom was made possible by improvements intransformer-baseddeepneural networks, particularlylarge language modelsLLMs. Major tools includechatbotssuch asChatGPT,DeepSeek,Copilot,Gemini, andLLaMA;text-to-imageartificial intelligence image generationsystems such asStable Diffusion,Midjourney, andDALL-E; andtext-to-videoAI generators such asSora.174175176177Technology companies developing generative AI includeOpenAI,Anthropic,Microsoft,Google,DeepSeek, andBaidu.178179180 Artificial intelligent AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, includingvirtual assistants,chatbots,autonomous vehicles,game-playing systems, andindustrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.184185186 Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer prediction,187AI-integrated sex toys e.g.,teledildonics,188AI-generated sexual education content,189and AI agents that simulate sexual and romantic partners e.g.,Replika.190AI is also used for the production of non-consensualdeepfake pornography, raising significant ethical and legal concerns.191 AI technologies have also been used to attempt to identifyonline gender-based violenceand onlinesexual groomingof minors.192193 There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated ""AI"" in some offerings or processes.194A few examples areenergy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions,foreign policy, or supply chain management. AI applications for evacuation anddisastermanagement are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.195196197 In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conductpredictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights."" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the2024 Indian elections, US50 million was spent on authorized AI-generated content, notably by creatingdeepfakesof allied including sometimes deceased politicians to better engage with voters, and by translating speeches to various local languages.198 AI has potential benefits and potential risks.199AI may be able to advance science and find solutions for serious problems:Demis HassabisofDeepMindhopes to ""solve intelligence, and then use that to solve everything else"".200However, as the use of AI has become widespread, several unintended consequences and risks have been identified.201In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.202 Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns aboutprivacy,surveillanceandcopyright. AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio.203For example, in order to buildspeech recognitionalgorithms,Amazonhas recorded millions of private conversations and allowedtemporary workersto listen to and transcribe some of them.204Opinions about this widespread surveillance range from those who see it as anecessary evilto those for whom it is clearlyunethicaland a violation of theright to privacy.205 AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such asdata aggregation,de-identificationanddifferential privacy.206Since 2016, some privacy experts, such asCynthia Dwork, have begun to view privacy in terms offairness.Brian Christianwrote that experts have pivoted ""from the question of 'what they know' to the question of 'what they're doing with it'.""207 Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of ""fair use"". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"".208209Website owners who do not wish to have their content scraped can indicate it in a ""robots.txt"" file.210In 2023, leading authors includingJohn GrishamandJonathan Franzen sued AI companies for using their work to train generative AI.211212Another discussed approach is to envision a separatesui generissystem of protection for creations generated by AI to ensure fair attribution and compensation for human authors.213 The commercial AI scene is dominated byBig Techcompanies such asAlphabet Inc.,Amazon,Apple Inc.,Meta Platforms, andMicrosoft.214215216Some of these players already own the vast majority of existingcloud infrastructureandcomputingpower fromdata centers, allowing them to entrench further in the marketplace.217218 In January 2024, theInternational Energy AgencyIEA releasedElectricity 2024, Analysis and Forecast to 2026, forecasting electric power use.219This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.220 Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms e.g., Microsoft, Meta, Google, Amazon into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources  from nuclear energy to geothermal to fusion. The tech firms argue that  in the long view  AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and ""intelligent"", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.221 A 2024Goldman SachsResearch Paper,AI Data Centers and the Coming US Power Demand Surge, found ""US power demand is likely to experience growth not seen in a generation...."" and forecasts that, by 2030, US data centers will consume 8 of US power, as opposed to 3 in 2022, presaging growth for the electrical power generation industry by a variety of means.222Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.223 In 2024, theWall Street Journalreported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for 650 Million US.224NvidiaCEOJen-Hsun Huangsaid nuclear power is a good option for the data centers.225 In September 2024,Microsoftannounced an agreement withConstellation Energyto re-open theThree Mile Islandnuclear power plant to provide Microsoft with 100 of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the USNuclear Regulatory Commission. If approved this will be the first ever US re-commissioning of a nuclear plant, over 835 megawatts of power  enough for 800,000 homes  of energy will be produced. The cost for re-opening and upgrading is estimated at 1.6 billion US and is dependent on tax breaks for nuclear power contained in the 2022 USInflation Reduction Act.226The US government and the state of Michigan are investing almost 2 billion US to reopen thePalisades Nuclearreactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO ofExelonwho was responsible for Exelon spinoff of Constellation.227 After the last approval in September 2023,Taiwansuspended the approval of data centers north ofTaoyuanwith a capacity of more than 5 MW in 2024, due to power supply shortages.228Taiwan aims tophase out nuclear powerby 2025.228On the other hand,Singaporeimposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.228 Although most nuclear plants in Japan have been shut down after the 2011Fukushima nuclear accident, according to an October 2024Bloombergarticle in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI.229Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.229 On 1 November 2024, theFederal Energy Regulatory CommissionFERC rejected an application submitted byTalen Energyfor approval to supply some electricity from the nuclear power stationSusquehannato Amazon's data center.230According to the Commission ChairmanWillie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.230 In 2025 a report prepared by the International Energy Agency estimated thegreenhouse gas emissionsfrom the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300-500 million tonnes depending on what measures will be taken. This is below 1.5 of the energy sector emissions. The emissions reduction potential of AI was estimated at 5 of the energy sector emissions, butrebound effectsfor example if people will pass from public transport to autonomous cars can reduce it.231 YouTube,Facebookand others userecommender systemsto guide users to more content. These AI programs were given the goal ofmaximizinguser engagement that is, the only goal was to keep people watching. The AI learned that users tended to choosemisinformation,conspiracy theories, and extremepartisancontent, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people intofilter bubbleswhere they received multiple versions of the same misinformation.232This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.233The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.234 In 2022,generative AIbegan to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.235One such potential malicious use is deepfakes forcomputational propaganda236. AI pioneerGeoffrey Hintonexpressed concern about AI enabling ""authoritarian leaders to manipulate their electorates"" on a large scale, among other risks.237 Machine learning applications will bebiasedkif they learn from biased data.239The developers may not be aware that the bias exists.240Bias can be introduced by the waytraining datais selected and by the way a model is deployed.241239If a biased algorithm is used to make decisions that can seriouslyharmpeople as it can inmedicine,finance,recruitment,housingorpolicing then the algorithm may causediscrimination.242The field offairnessstudies how to prevent harms from algorithmic biases. On June 28, 2015,Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as ""gorillas"" because they were black. The system was trained on a dataset that contained very few images of black people,243a problem called ""sample size disparity"".244Google ""fixed"" this problem by preventing the system from labellinganythingas a ""gorilla"". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.245 COMPASis a commercial program widely used byU.S. courtsto assess the likelihood of adefendantbecoming arecidivist. In 2016,Julia AngwinatProPublicadiscovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61, the errors for each race were differentthe system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.246In 2017, several researcherslshowed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.248 A program can make biased decisions even if the data does not explicitly mention a problematic feature such as ""race"" or ""gender"". The feature will correlate with other features like ""address"", ""shopping history"" or ""first name"", and the program will make the same decisions based on these features as it would on ""race"" or ""gender"".249Moritz Hardt said ""the most robust fact in this research area is that fairness through blindness doesn't work.""250 Criticism of COMPAS highlighted that machine learning models are designed to make ""predictions"" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions asrecommendations, some of these ""recommendations"" will likely be racist.251Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will bebetterthan the past. It is descriptive rather than prescriptive.m Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4 are black and 20 are women.244 There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category isdistributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negativestereotypesor render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict withanti-discrimination laws.238 At its 2022Conference on Fairness, Accountability, and TransparencyACM FAccT 2022, theAssociation for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.dubiousdiscuss253 Many AI systems are so complex that their designers cannot explain how they reach their decisions.254Particularly withdeep neural networks, in which there are a large amount of non-linearrelationships between inputs and outputs. But some popular explainability techniques exist.255 It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with aruleras ""cancerous"", because pictures of malignancies typically include a ruler to show the scale.256Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at ""low risk"" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.257 People who have been harmed by an algorithm's decision have a right to an explanation.258Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union'sGeneral Data Protection Regulationin 2016 included an explicit statement that this right exists.nIndustry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.259 DARPAestablished theXAI""Explainable Artificial Intelligence"" program in 2014 to try to solve these problems.260 Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.261LIME can locally approximate a model's outputs with a simpler, interpretable model.262Multitask learningprovides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.263Deconvolution,DeepDreamand othergenerativemethods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.264Forgenerative pre-trained transformers,Anthropicdeveloped a technique based ondictionary learningthat associates patterns of neuron activations with human-understandable concepts.265 Artificial intelligence provides a number of tools that are useful tobad actors, such asauthoritarian governments,terrorists,criminalsorrogue states. A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.oWidely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentiallyweapons of mass destruction.267Even when used in conventional warfare, they currently cannot reliably choose targets and could potentiallykill an innocent person.267In 2014, 30 nations including China supported a ban on autonomous weapons under theUnited Nations'Convention on Certain Conventional Weapons, however theUnited Statesand others disagreed.268By 2015, over fifty countries were reported to be researching battlefield robots.269 AI tools make it easier forauthoritarian governmentsto efficiently control their citizens in several ways.Faceandvoice recognitionallow widespreadsurveillance.Machine learning, operating this data, canclassifypotential enemies of the state and prevent them from hiding.Recommendation systemscan precisely targetpropagandaandmisinformationfor maximum effect.Deepfakesandgenerative AIaid in producing misinformation. Advanced AI can make authoritariancentralized decision makingmore competitive than liberal and decentralized systems such asmarkets. It lowers the cost and difficulty ofdigital warfareandadvanced spyware.270All these technologies have been available since 2020 or earlierAIfacial recognition systemsare already being used formass surveillancein China.271272 There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.273 Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.274 In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI.275A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-termunemployment, but they generally agree that it could be a net benefit ifproductivitygains areredistributed.276Risk estimates vary; for example, in the 2010s, Michael Osborne andCarl Benedikt Freyestimated 47 of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9 of U.S. jobs as ""high risk"".p278The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.274In April 2023, it was reported that 70 of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.279280 Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence;The Economiststated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"".281Jobs at extreme risk range fromparalegalsto fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.282 From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward byJoseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.283 It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicistStephen Hawkingstated, ""spell the end of the human race"".284This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like ""self-awareness"" or ""sentience"" or ""consciousness"" and becomes a malevolent character.qThese sci-fi scenarios are misleading in several ways. First, AI does not require human-likesentienceto be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. PhilosopherNick Bostromargued that if one givesalmost anygoal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it he used the example of apaperclip factory manager.286Stuart Russellgives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that ""you can't fetch the coffee if you're dead.""287In order to be safe for humanity, asuperintelligencewould have to be genuinelyalignedwith humanity's morality and values so that it is ""fundamentally on our side"".288 Second,Yuval Noah Harariargues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things likeideologies,law,government,moneyand theeconomyare built onlanguage; they exist because there are stories that billions of people believe. The current prevalence ofmisinformationsuggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.289 The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.290Personalities such asStephen Hawking,Bill Gates, andElon Musk,291as well as AI pioneers such asYoshua Bengio,Stuart Russell,Demis Hassabis, andSam Altman, have expressed concerns about existential risk from AI. In May 2023,Geoffrey Hintonannounced his resignation from Google in order to be able to ""freely speak out about the risks of AI"" without ""considering how this impacts Google"".292He notably mentioned risks of anAI takeover,293and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.294 In 2023, many leading AI experts endorsedthe joint statementthat ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"".295 Some other researchers were more optimistic. AI pioneerJ√ºrgen Schmidhuberdid not sign the joint statement, emphasising that in 95 of all cases, AI research is about making ""human lives longer and healthier and easier.""296While the tools that are now being used to improve lives can also be used by bad actors, ""they can also be used against the bad actors.""297298Andrew Ngalso argued that ""it's a mistake to fall for the doomsday hype on AIand that regulators who do will only benefit vested interests.""299Yann LeCun""scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.""300In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.301However, after 2016, the study of current and future risks and possible solutions became a serious area of research.302 Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans.Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.303 Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.304The field of machine ethics is also called computational morality,304and was founded at anAAAIsymposium in 2005.305 Other approaches includeWendell Wallach's ""artificial moral agents""306andStuart J. Russell'sthree principlesfor developing provably beneficial machines.307 Active organizations in the AI open-source community includeHugging Face,308Google,309EleutherAIandMeta.310Various AI models, such asLlama 2,MistralorStable Diffusion, have been made open-weight,311312meaning that their architecture and trained parameters the ""weights"" are publicly available. Open-weight models can be freelyfine-tuned, which allows companies to specialize them with their own data and for their own use-case.313Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities such as the potential to drastically facilitatebioterrorism and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.314 Artificial Intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by theAlan Turing Instituteand based on the SUM values, outlines four main ethical dimensions, defined as follows:315316 Other developments in ethical frameworks include those decided upon during theAsilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;317however, these principles are not without criticism, especially regards to the people chosen to contribute to these frameworks.318 Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.319 TheUK AI Safety Institutereleased in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.320 The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.321The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.322According to AI Index atStanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.323324Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.325Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.325TheGlobal Partnership on Artificial Intelligencewas launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.325Henry Kissinger,Eric Schmidt, andDaniel Huttenlocherpublished a joint statement in November 2021 calling for a government commission to regulate AI.326In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.327In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.328In 2024, theCouncil of Europecreated the first international legally binding treaty on AI, called the ""Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law"". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.329 In a 2022Ipsossurvey, attitudes towards AI varied greatly by country; 78 of Chinese citizens, but only 35 of Americans, agreed that ""products and services using AI have more benefits than drawbacks"".323A 2023ReutersIpsos poll found that 61 of Americans agree, and 22 disagree, that AI poses risks to humanity.330In a 2023Fox Newspoll, 35 of Americans thought it ""very important"", and an additional 41 thought it ""somewhat important"", for the federal government to regulate AI, versus 13 responding ""not very important"" and 8 responding ""not at all important"".331332 In November 2023, the first globalAI Safety Summitwas held inBletchley Parkin the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.33328 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.334335In May 2024 at theAI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.336337 The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly toAlan Turing'stheory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable form of mathematical reasoning.339340This, along with concurrent discoveries incybernetics,information theoryandneurobiology, led researchers to consider the possibility of building an ""electronic brain"".rThey developed several areas of research that would become part of AI,342such asMcCullouchandPittsdesign for ""artificial neurons"" in 1943,115and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced theTuring testand showed that ""machine intelligence"" was plausible.343340 The field of AI research was founded ata workshopatDartmouth Collegein 1956.s6The attendees became the leaders of AI research in the 1960s.tThey and their students produced programs that the press described as ""astonishing"":ucomputers were learningcheckersstrategies, solving word problems in algebra, provinglogical theoremsand speaking English.v7Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.340 Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine withgeneral intelligenceand considered this the goal of their field.347In 1965Herbert Simonpredicted, ""machines will be capable, within twenty years, of doing any work a man can do"".348In 1967Marvin Minskyagreed, writing that ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".349They had, however, underestimated the difficulty of the problem.wIn 1974, both the U.S. and British governments cut off exploratory research in response to thecriticismofSir James Lighthill351and ongoing pressure from the U.S. Congress tofund more productive projects.352Minsky's andPapert's bookPerceptronswas understood as proving thatartificial neural networkswould never be useful for solving real-world tasks, thus discrediting the approach altogether.353The ""AI winter"", a period when obtaining funding for AI projects was difficult, followed.9 In the early 1980s, AI research was revived by the commercial success ofexpert systems,354a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan'sfifth generation computerproject inspired the U.S. and British governments to restore funding foracademic research.8However, beginning with the collapse of theLisp Machinemarket in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.10 Up to this point, most of AI's funding had gone to projects that used high-levelsymbolsto representmental objectslike plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especiallyperception,robotics,learningandpattern recognition,355and began to look into ""sub-symbolic"" approaches.356Rodney Brooksrejected ""representation"" in general and focussed directly on engineering machines that move and survive.xJudea Pearl,Lofti Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.86361But the most important development was the revival of ""connectionism"", including neural network research, byGeoffrey Hintonand others.362In 1990,Yann LeCunsuccessfully showed thatconvolutional neural networkscan recognize handwritten digits, the first of many successful applications of neural networks.363 AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields such asstatistics,economicsandmathematics.364By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"" a tendency known as theAI effect.365However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield ofartificial general intelligenceor ""AGI"", which had several well-funded institutions by the 2010s.4 Deep learningbegan to dominate industry benchmarks in 2012 and was adopted throughout the field.11For many specific tasks, other methods were abandoned.yDeep learning's success was based on both hardware improvements faster computers,367graphics processing units,cloud computing368 and access tolarge amounts of data369including curated datasets,368such asImageNet. Deep learning's success led to an enormous increase in interest and funding in AI.zThe amount of machine learning research measured by total publications increased by 50 in the years 20152019.325 In 2016, issues offairnessand the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. Thealignment problembecame a serious field of academic study.302 In the late 2010s and early 2020s,AGIcompanies began to deliver programs that created enormous interest. In 2015,AlphaGo, developed byDeepMind, beat the world championGo player. The program taught only the game's rules and developed a strategy by itself.GPT-3is alarge language modelthat was released in 2020 byOpenAIand is capable of generating high-quality human-like text.370ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.371It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.372These programs, and others, inspired an aggressiveAI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about 50 billion annually was invested in ""AI"" around 2022 in the U.S. alone and about 20 of the new U.S. Computer Science PhD graduates have specialized in ""AI"".373About 800,000 ""AI""-related U.S. job openings existed in 2022.374According to PitchBook research, 22 of newly fundedstartupsin 2024 claimed to be AI companies.375 Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.376Another major focus has been whether machines can be conscious, and the associated ethical implications.377Many other topics in philosophy are relevant to AI, such asepistemologyandfree will.378Rapid advancements have intensified public discussions on the philosophy andethics of AI.377 Alan Turingwrote in 1950 ""I propose to consider the question 'can machines think'?""379He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".379He devised the Turing test, which measures the ability of a machine to simulate human conversation.343Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes thatwe can not determine these things about other peoplebut ""it is usual to have a polite convention that everyone thinks.""380 RussellandNorvigagree with Turing that intelligence must be defined in terms of external behavior, not internal structure.1However, they are critical that the test requires the machine to imitate humans. ""Aeronautical engineeringtexts"", they wrote, ""do not define the goal of their field as making 'machines that fly so exactly likepigeonsthat they can fool other pigeons.'""382AI founderJohn McCarthyagreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".383 McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world"".384Another AI founder,Marvin Minsky, similarly describes it as ""the ability to solve hard problems"".385The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.1These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machineand no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google,386a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,387with many companies during the early 2020s AI boom using the term as a marketingbuzzword, often even if they did ""not actually use AI in a material way"".388 No established unifying theory orparadigmhas guided AI research for most of its history.aaThe unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks"". This approach is mostlysub-symbolic,softandnarrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AIor ""GOFAI""390simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed thephysical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""391 However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning.Moravec's paradoxis the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.392PhilosopherHubert Dreyfushadarguedsince the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.393Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.ab16 The issue is not resolved:sub-symbolicreasoning can make many of the same inscrutable mistakes that human intuition does, such asalgorithmic bias. Critics such asNoam Chomskyargue continuing research into symbolic AI will still be necessary to attain general intelligence,395396in part because sub-symbolic AI is a move away fromexplainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field ofneuro-symbolic artificial intelligenceattempts to bridge the two approaches. ""Neats"" hope that intelligent behavior is described using simple, elegant principles such aslogic,optimization, orneural networks. ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,397but eventually was seen as irrelevant. Modern AI has elements of both. Finding a provably correct or optimal solution isintractablefor many important problems.15Soft computing is a set of techniques, includinggenetic algorithms,fuzzy logicand neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. AI researchers are divided as to whether to pursue the goals of artificial general intelligence andsuperintelligencedirectly or to solve as many specific problems as possible narrow AI in hopes these solutions will lead indirectly to the field's long-term goals.398399General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Thephilosophy of minddoes not know whether a machine can have amind,consciousnessandmental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence.RussellandNorvigadd that ""the additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.""400However, the question has become central to the philosophy of mind. It is also typically the central question at issue inartificial intelligence in fiction. David Chalmersidentified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness.401The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how thisfeelsor why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something Dennett's consciousness illusionism says this is an illusion. While humaninformation processingis easy to explain, humansubjective experienceis difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person toknow what red looks like.402 Computationalism is the position in thephilosophy of mindthat the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to themindbody problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophersJerry FodorandHilary Putnam.403 PhilosopherJohn Searlecharacterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""acSearle challenges this claim with hisChinese roomargument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.407 It is difficult or impossible to reliably evaluate whether an advancedAI is sentienthas the ability to feel, and if so, to what degree.408But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.409410Sapiencea set of capacities related to high intelligence, such as discernment orself-awareness may provide another moral basis for AI rights.409Robot rightsare also sometimes proposed as a practical way to integrate autonomous agents into society.411 In 2017, the European Union considered granting ""electronic personhood"" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.412Critics argued in 2018 that granting rights to AI systems would downplay the importance ofhuman rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.413414 Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be amoral blind spotanalogous toslaveryorfactory farming, which could lead tolarge-scale sufferingif sentient AI is created and carelessly exploited.410409 Asuperintelligenceis a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.399If research intoartificial general intelligenceproduced sufficiently intelligent software, it might be able toreprogram and improve itself. The improved software would be even better at improving itself, leading to whatI. J. Goodcalled an ""intelligence explosion"" andVernor Vingecalled a ""singularity"".415 However, technologies cannot improve exponentially indefinitely, and typically follow anS-shaped curve, slowing when they reach the physical limits of what the technology can do.416 Robot designerHans Moravec, cyberneticistKevin Warwickand inventorRay Kurzweilhave predicted that humans and machines may merge in the future intocyborgsthat are more capable and powerful than either. This idea, called transhumanism, has roots in the writings ofAldous HuxleyandRobert Ettinger.417 Edward Fredkinargues that ""artificial intelligence is the next step in evolution"", an idea first proposed bySamuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon byGeorge Dysonin his 1998 bookDarwin Among the Machines: The Evolution of Global Intelligence.418 Arguments fordecomputinghave been raised byDan McQuillanResisting AI: An Anti-fascist Approach to Artificial Intelligence, 2022, meaning an opposition to the sweeping application and expansion of artificial intelligence. Similar todegrowth, the approach criticizes AI as an outgrowth of the systemic issues and capitalist world we live in. It argues that a different future is possible, in which distance between people is reduced rather than increased through AI intermediaries.419 Thought-capable artificial beings have appeared as storytelling devices since antiquity,420and have been a persistent theme inscience fiction.421 A commontropein these works began withMary Shelley'sFrankenstein, where a human creation becomes a threat to its masters. This includes such works asArthur C. Clarke'sandStanley Kubrick's2001: A Space Odysseyboth 1968, withHAL 9000, the murderous computer in charge of theDiscovery Onespaceship, as well asThe Terminator1984 andThe Matrix1999. In contrast, the rare loyal robots such as Gort fromThe Day the Earth Stood Still1951 and Bishop fromAliens1986 are less prominent in popular culture.422 Isaac Asimovintroduced theThree Laws of Roboticsin many stories, most notably with the ""Multivac"" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;423while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.424 Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that havethe ability to feel, and thus to suffer. This appears inKarel ƒåapek'sR.U.R., the filmsA.I. Artificial IntelligenceandEx Machina, as well as the novelDo Androids Dream of Electric Sheep?, byPhilip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.425 The two most widely used textbooks in 2023 see theOpen Syllabus: The four most widely used AI textbooks in 2008: Other textbooks: TITLE: Artificial intelligence - Wikipedia",2025-05-06 13:42:24
https://en.wikipedia.org/wiki/Artificial_intelligence_(disambiguation),Artificial intelligence (disambiguation) - Wikipedia,Artificial intelligence disambiguation Contents Music Other uses See also Albums and EPs Songs Artificial intelligenceis the intelligence exhibited by machines and software. Artificial intelligencemay also refer to: TITLE: Artificial intelligence disambiguation - Wikipedia,2025-05-06 13:42:27
https://en.wikipedia.org/wiki/Artificial_intelligence_systems_integration,Artificial intelligence systems integration - Wikipedia,"Artificial intelligence systems integration Contents Integration focus Challenges and solutions Methodologies Examples See also References Notes External links Constructionist design methodology The core idea ofartificial intelligence systems integrationis making individualsoftware components, such asspeech synthesizers, interoperable with other components, such ascommon sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middlewareblackboard system. Mostartificial intelligencesystems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that ofspeech recognition. However, in recent years, there has been an increasing discussion on the importance ofsystems integrationas a field in its own right. Proponents of this approach are researchers such asMarvin Minsky,Aaron Sloman,Deb Roy,Kristinn R. Th√≥rissonandMichael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of relatively simple A.I. systems for specific problem domains such ascomputer vision,speech synthesis, etc., and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch. The focus on systems' integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes andor utilizemulti-modalinput and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical or some other undefined mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary. Collaboration is an integral part ofsoftware developmentas evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others such asW3Cstandards for webpage development. However, collaboration in fields of A.I. has been lacking, for the most part not seen outside the respected schools, departments or research institutes and sometimes not within them either. This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the ""not invented here"" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others. The outcome of this in A.I. is a large set of ""solution islands"": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples: With the increased popularity of thefree software movement, a lot of the software being created, including A.I. systems, is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components that often serve the same purpose have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module, which can then be tried in various settings and configurations of larger architectures. Some challenging and limitations of using A.I. software is the uncontrolled fatal errors. For example, serious and fatal errors have been discovered in very precise fields such as human oncology, as in an article published in the journal Oral Oncology Reports entitled When AI goes wrong: Fatal errors in oncological research reviewing assistance"".1The article pointed out a grave error in artificial intelligence based on GBT in the field of biophysics. Many online communities for A.I. developers exist where tutorials, examples, and forums aim at helping both beginners and experts build intelligent systems. However, few communities have succeeded in making a certain standard, or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with ease. Theconstructionist design methodologyCDM, or 'Constructionist A.I.' is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires the integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol see below was inspired by the CDM and has frequently been used to aid in the development of intelligent systems using CDM. TITLE: Artificial intelligence systems integration - Wikipedia",2025-05-06 13:42:29
https://en.wikipedia.org/wiki/Artificial_intelligence_art,Artificial intelligence art - Wikipedia,"Artificial intelligence art Contents History Tools and processes Impact Analysis of existing art using AI Other forms of art See also References Early history Artistic history Technical history Imagery Prompt engineering and sharing Bias Copyright Deception Income and employment stability Power usage Artificial intelligence artisvisual artworkcreated or enhanced through the use ofartificial intelligenceAI programs. Artists began to create artificial intelligence art in the mid to late 20th century when the discipline was founded. Throughoutits history, artificial intelligence art has raised manyphilosophicalconcerns related to thehuman mind, artificial beings, and what can be considered art in a humanAI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.1 During theAI boomof the 2020s,text-to-image modelssuch asMidjourney,DALL-E,Stable Diffusion, andFLUX.1became widely available to the public, allowing users to quickly generate imagery with little effort.23Commentary about AI art in the 2020s has often focused on issues related tocopyright,deception,defamation, and its impact on more traditional artists, includingtechnological unemployment. Audience reception shows bias against AI-generated art, where people tend to value artworks less when they know they were created with AI.4Opinions have also risen on the possible effect AI generated art might have on creativity. Automated art dates back at least to theautomataofancient Greek civilization, when inventors such asDaedalusandHero of Alexandriawere described as designing machines capable of writing text, generating sounds, and playing music.56Creative automatons have flourished throughout history, such asMaillardet's automaton, created around 1800 and capable of creating multiple drawings and poems.7 Also in the 19th century,Ada Lovelace, writes that ""computing operations"" could be used to generate music and poems, now referred to as ""The Lovelace Effect,"" where a computer's behavior is viewed as creative.8Lovelace also discusses a concept known as ""The Lovelace Objection,"" where she argues that a machine has ""no pretensions whatever to originate anything.""9 In 1950, with the publication ofAlan Turing's paper ""Computing Machinery and Intelligence"", there was a shift from defining machine intelligence in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly.10Shortly after, the academic discipline of artificial intelligence was founded at a researchworkshopatDartmouth Collegein 1956.11Since its founding, researchers in the field have explored philosophical questions about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored bymyth,fiction, andphilosophysince antiquity.12 Since the founding of AI in the 1950s, artists have used artificial intelligence to create artistic works. These works were sometimes referred to asalgorithmic art,13computer art,digital art, ornew media art.14 One of the first significant AI art systems isAARON, developed byHarold Cohenbeginning in the late 1960s at theUniversity of Californiaat San Diego.15AARON uses a symbolic rule-based approach to generate technical images in the era ofGOFAIprogramming, and it was developed by Cohen with the goal of being able to code the act of drawing.16AARON was exhibited in 1972 at theLos Angeles County Museum of Art.17From 1973 to 1975, Cohen refined AARON during a residency at theArtificial Intelligence LaboratoryatStanford University.18In 2024, theWhitney Museum of American Artexhibited AI art from throughout Cohen's career, including re-created versions of his early robotic drawing machines.18 Karl Simshas exhibited art created withartificial lifesince the 1980s. He received an M.S. in computer graphics from theMIT Media Labin 1987 and was artist-in-residence from 1990 to 1996 at thesupercomputermanufacturer and artificial intelligence companyThinking Machines.192021In both 1991 and 1992, Sims won the Golden Nica award atPrix Ars Electronicafor his videos using artificial evolution.222324In 1997, Sims created the interactive artificial evolution installationGal√°pagosfor theNTT InterCommunication Centerin Tokyo.25Sims received anEmmy Awardin 2019 for outstanding achievement in engineering development.26 In 1999,Scott Dravesand a team of several engineers created and releasedElectric Sheepas afree softwarescreensaver.27Electric Sheepis a volunteer computing project for animating and evolvingfractal flames, which are distributed to networked computers which display them as a screensaver. The screensaver used AI to create an infinite animation by learning from its audience. In 2001, Draves won the Fundacion Telef√≥nica Life 4.0 prize forElectric Sheep.28unreliable source? In 2014,Stephanie Dinkinsbegan working onConversations with Bina48.29For the series, Dinkins recorded her conversations withBINA48, a social robot that resembles a middle-aged black woman.3031In 2019, Dinkins won theCreative Capitalaward for her creation of an evolving artificial intelligence based on the ""interests and cultures of people of color.""32 In 2015,Sougwen ChungbeganMimicry Drawing Operations Unit: Generation 1, an ongoing collaboration between the artist and a robotic arm.33In 2019, Chung won theLumen Prizefor her continued performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.34 In 2018, an auction sale of artificial intelligence art was held atChristie'sin New York where the AI artworkEdmond de Belamysold forUS432,500, which was almost 45 times higher than its estimate of US7,00010,000. The artwork was created by Obvious, a Paris-based collective.353637 In 2024, Japanese filmgenerAIdoscopewas released. The film was co-directed byHirotaka Adachi, Takeshi Sone, and Hiroki Yamaguchi. All video, audio, and music in the film were created with artificial intelligence.38 In 2025, Japaneseanimetelevision seriesTwins Hinahimawas released. The anime was produced and animated with AI assistance during the process of cutting and conversion of photographs into anime illustrations and later retouched by art staff. Most of the remaining parts such as characters and logos were hand-drawn with various software.3940 Deep learning, characterized by its multi-layer structure that attempts to mimic the human brain, first came about in the 2010s and causing a significant shift in the world of AI art.41During the deep learning era, there are mainly these types of designs for generative art:autoregressive models,diffusion models,GANs,normalizing flows. In 2014,Ian Goodfellowand colleagues atUniversit√© de Montr√©aldeveloped thegenerative adversarial networkGAN, a type ofdeep neural networkcapable of learning to mimic thestatistical distributionof input data such as images. The GAN uses a ""generator"" to create new images and a ""discriminator"" to decide which created images are considered successful.42Unlike previous algorithmic art that followed hand-coded rules, generative adversarial networks could learn a specificaestheticby analyzing adatasetof example images.13 In 2015, a team atGooglereleasedDeepDream, a program that uses aconvolutional neural networkto find and enhance patterns in images via algorithmicpareidolia.434445The process creates deliberately over-processed images with a dream-like appearance reminiscent of apsychedelic experience.46Later, in 2017, a conditional GAN learned to generate 1000 image classes ofImageNet, a large visualdatabasedesigned for use invisual object recognition softwareresearch.4748By conditioning the GAN on both random noise and a specific class label, this approach enhanced the quality of image synthesis for class-conditional models.49 Autoregressive modelswere used for image generation, such as PixelRNN 2016, which autoregressively generates one pixel after another with arecurrent neural network.50Immediately after theTransformerarchitecture was proposed inAttention Is All You Need2018, it was used for autoregressive generation of images, but without text conditioning.51 The websiteArtbreeder, launched in 2018, uses the modelsStyleGANand BigGAN5253to allow users to generate and modify images such as faces, landscapes, and paintings.54 In the 2020s,text-to-image models, which generate images based onprompts, became widely used, marking yet another shift in the creation of AI generated artworks.2 In 2021, using the influentiallarge languagegenerative pre-trained transformermodels that are used inGPT-2andGPT-3,OpenAIreleased a series of images created with the text-to-image AI modelDALL-E 1.55It was an autoregressive generative model with essentially the same architecture as GPT-3. Along with this, later in 2021,EleutherAIreleased theopen sourceVQGAN-CLIP56based on OpenAI's CLIP model.57Diffusion models, generative models used to create synthetic data based on existing data,58were first proposed in 2015,59but they only became better than GANs in early 2021.60Latent diffusion modelwas published in December 2021 and became the basis for the laterStable DiffusionAugust 2022.61 In 2022,Midjourney62was released, followed byGoogle Brain'sImagenand Parti, which were announced in May 2022,Microsoft's NUWA-Infinity,632and thesource-availableStable Diffusion, which was released in August 2022.646566DALL-E2, a successor to DALL-E, was beta-tested and released with the further successor DALL-E3 being released in 2023. Stability AI has a Stable Diffusion web interface called DreamStudio,67plugins forKrita,Photoshop,Blender, andGIMP,68and theAutomatic1111web-based open sourceuser interface.697071Stable Diffusion's main pre-trained model is shared on theHugging Face Hub.72 Ideogramwas released in August 2023, this model is known for its ability to generate legible text.7374 In 2024,Fluxwas released. This model can generate realistic images and was integrated intoGrok, the chatbot used onX formerly Twitter, andLe Chat, the chatbot ofMistral AI.3757677Flux was developed by Black Forest Labs, founded by the researchers behind Stable Diffusion.78Grok later switched to its own text-to-image modelAurorain December of the same year.79Several companies, along with their products, have also developed an AI model integrated with an image editing service.Adobehas released and integrated the AI modelFireflyintoPremiere Pro,Photoshop, andIllustrator.8081Microsoft has also publicly announced AI image-generator features forMicrosoft Paint.82Along with this, some examples oftext-to-video modelsof the mid-2020s areRunway's Gen-2, Google'sVideoPoet, and OpenAI'sSora, which was released in December 2024.8384 There are many tools available to the artist when working with diffusion models. They can define both positive and negative prompts, but they are also afforded a choice in using or omitting the use ofVAEs,LoRAs, hypernetworks, IP-adapter, and embeddingtextual inversions. Artists can tweak settings like guidance scale which balances creativity and accuracy, seed to control randomness, and upscalers to enhance image resolution, among others. Additional influence can be exerted during pre-inference by means of noise manipulation, while traditional post-processing techniques are frequently used post-inference. People can also train their own models. In addition, procedural ""rule-based"" generation of images using mathematical patterns, algorithms that simulate brush strokes and other painted effects, and deep learning algorithms such as generative adversarial networks GANs and transformers have been developed. Several companies have released apps and websites that allow one to forego all the options mentioned entirely while solely focusing on the positive prompt. There also exist programs which transform photos into art-like images in the style of well-known sets of paintings.8586 There are many options, ranging from simple consumer-facing mobile apps toJupyternotebooks and web UIs that require powerful GPUs to run effectively.87Additional functionalities include ""textual inversion,"" which refers to enabling the use of user-provided concepts like an object or a style learned from a few images. Novel art can then be generated from the associated words the text that has been assigned to the learned, often abstract, concept8889and model extensions or fine-tuning such asDreamBooth. AI has the potential for asocietal transformation, which may include enabling the expansion of noncommercial niche genres such ascyberpunk derivativeslikesolarpunk by amateurs, novel entertainment, fast prototyping,90increasing art-making accessibility,90and artistic output per effort or expenses or time90e.g., via generating drafts, draft-definitions, and image components inpainting. Generated images are sometimes used as sketches,91low-cost experiments,92inspiration, or illustrations ofproof-of-concept-stage ideas. Additional functionalities or improvements may also relate to post-generation manual editing i.e., polishing, such as subsequent tweaking with an image editor.92 Promptsfor some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like ""in the style of name of an artist"" in the prompt93or selection of a broad aestheticart style.9491There are platforms for sharing, trading, searching, forkingrefining, or collaborating on prompts for generating specific imagery from image generators.95969798Prompts are often shared along with images onimage-sharingwebsites such asRedditand AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image; additional inputs that determine the generated image include theoutput resolution,random seed, and random sampling parameters.99 Synthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years.90Harvard Kennedy Schoolresearchers voiced concerns about synthetic media serving as a vector for political misinformation soon after studying the proliferation of AI art on the X platform.100Synthographyis a proposed term for the practice of generating images that are similar to photographs using AI.101 A major concern raised about AI-generated images and art issampling biaswithin model training data leading towards discriminatory output from AI art models. In 2023,University of Washingtonresearchers found evidence of racial bias within the Stable Diffusion model, with images of a ""person"" corresponding most frequently with images of males from Europe or North America.102 Looking more into thesampling biasfound within AI training data, in 2017, researchers at Princeton University used AI software to link over 2 million words, finding that European names were viewed as more ""pleasant"" than African-Americans names, and that the words ""woman"" and ""girl"" were more likely to be associated with the arts instead of science and math, ""which were most likely connected to males.""103Generative AI models typically work based on user-entered word-based prompts, especially in the case ofdiffusion models, and this word-related bias may lead to biased results. Along with this, generative AI can perpetuate harmful stereotypes regarding women. For example,Lensa, an AI app that trended onTikTokin 2023, was known to lighten black skin, make users thinner, and generate hypersexualized images of women.104Melissa Heikkil√§, a senior reporter atMIT Technology Review, shared the findings of an experiment using Lensa, noting that the generated avatars did not resemble her and often depicted her in a hypersexualized manner.105Experts suggest that such outcomes can result from biases in the datasets used to train AI models, which can sometimes contain imbalanced representations, including hypersexual or nude imagery.106107 In 2024, Google'schatbotGemini's AI image generator was criticized for perceivedracial bias, with claims that Gemini deliberately underrepresented white people in its results.108Users reported that it generated images of white historical figures like theFounding Fathers,Nazi soldiers, andVikingsas other races, and that it refused to process prompts such as ""happy white people"" and ""idealnuclear family"".108109Google later apologized for ""missing the mark"" and took Gemini's image generator offline for updates.110This prompted discussions about the ethical implications111of representing historical figures through a contemporary lens, leading critics to argue that these outputs could mislead audiences regarding actual historical contexts.112In addition to the well-documented representational issues such as racial and gender bias, some scholars have also pointed out deeper conceptual assumptions that shape how we perceive AI-generated art. For instance, framing AI strictly as a passive tool overlooks how cultural and technological factors influence its outputs. Others suggest viewing AI as part of a collaborative creative process, where both human and machine contribute to the artistic result.113 Legal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century. Some artists use AI art to critique and explore the ethics of usinggathered datato produce new artwork.114 In 1985, intellectual property law professorPamela Samuelsonargued thatUS copyrightshould allocate algorithmically generated artworks to the user of the computer program.115A 2019Florida Law Reviewarticle presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define ""author"" as a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the ""work for hire"" doctrine, under which ownership of a copyright is transferred to the ""employer."" In the third situation, copyright assignments would never take place, and such works would be in thepublic domain, as copyright assignments require an act of authorship.116 In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation.117In September 2022, Reema Selhi, of theDesign and Artists Copyright Society, stated that ""there are no safeguards for artists to be able to identify works in databases that are being used and opt out.""118Some have claimed that images generated with these models can bear resemblance to extant artwork, sometimes including the remains of the original artist's signature.118119In December 2022, users of the portfolio platform ArtStation staged an online protest against non-consensual use of their artwork within datasets; this resulted in opt-out services, such as ""Have I Been Trained?"" increasing in profile, as well as some online art platforms promising to offer their own opt-out options.120According to theUS Copyright Office, artificial intelligence programs are unable to hold copyright,121122123a decision upheld at the Federal District level as of August 2023 followed the reasoning from themonkey selfie copyright dispute.124 OpenAI, the developer ofDALL-E, has its own policy on who owns generated art. They assign the right and title of a generated image to the creator, meaning the user who inputted the prompt owns the image generated, along with the right to sell, reprint, and merchandise it.125 In January 2023, three artistsSarah Andersen,Kelly McKernan, and Karla Ortizfiled acopyright infringementlawsuit against Stability AI,Midjourney, andDeviantArt, claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web.126In July 2023, U.S. District JudgeWilliam Orrickwas inclined to dismiss most of the lawsuits filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint.127Also in 2023, Stability AI was sued byGetty Imagesfor using its images in the training data.128A tool built bySimon Willisonallowed people to search 0.5 of the training data for Stable Diffusion V1.1, i.e., 12 million of the 2.3 billion instances fromLAION2B. Artist Karen Hallion discovered that her copyrighted images were used as training data without their consent.129 In March 2024, Tennessee enacted theELVIS Act, which prohibits the use of AI to mimic a musician's voice without permission.130A month later in that year,Adam Schiffintroduced theGenerative AI Copyright Disclosure Actwhich, if passed, would require that AI companies to submit copyrighted works in their datasets to theRegister of Copyrightsbefore releasing new generative AI systems.131In November 2024, a group of artists and activists shared early access to OpenAIs unreleased video generation model,Sora, viaHuggingface. The action, accompanied by a statement, criticized the exploitative use of artists work by major corporations.'132133134 As with other types ofphoto manipulationsince the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading and can be made to damage a person's reputation, such asdeepfakes.135ArtistSarah Andersen, who previously had her art copied and edited to depictNeo-Nazibeliefs, stated that the spread ofhate speechonline can be worsened by the use of image generators.129Some also generate images or videos for the purpose ofcatfishing. AI systems have the ability to create deepfake content, which is often viewed as harmful and offensive. The creation of deepfakes poses a risk to individuals who have not consented to it.136This mainly refers todeepfake pornographywhich is used asrevenge porn, where sexually explicit material is disseminated to humiliate or harm another person. AI-generatedchild pornographyhas been deemed a potential danger to society due to its unlawful nature.137 After winning the 2023 ""Creative"" ""Open competition"" Sony World Photography Awards, Boris Eldagsen stated that his entry was actually created with artificial intelligence. Photographer Feroz Khan commented to theBBCthat Eldagsen had ""clearly shown that even experienced photographers and art experts can be fooled"".139Smaller contests have been affected as well; in 2023, a contest run by authorMark LawrenceasSelf-Published Fantasy Blog-Offwas cancelled after the winning entry was allegedly exposed to be a collage of images generated with Midjourney.140 In May 2023, on social media sites such as Reddit and Twitter, attention was given to a Midjourney-generated image ofPope Franciswearing a white puffer coat.141142Additionally, an AI-generated image of an attack on thePentagonwent viral as part of ahoax news storyon Twitter.143144 In the days beforeMarch 2023 indictment of Donald Trumpas part of theStormy DanielsDonald Trump scandal, several AI-generated images allegedly depicting Trump's arrest went viral online.145146On March 20, British journalistEliot Higginsgenerated various images of Donald Trump being arrested or imprisoned using Midjourney v5 and posted them on Twitter; two images of Trump struggling against arresting officers went viral under the mistaken impression that they were genuine, accruing more than 5 million views in three days.147148According to Higgins, the images were not meant to mislead, but he was banned from using Midjourney services as a result. As of April 2024, the tweet had garnered more than 6.8 million views. In February 2024, the paperCellular functions of spermatogonial stem cells in relation to JAKSTAT signaling pathwaywas published using AI-generated images. It was later retracted fromFrontiers in Cell and Developmental Biologybecause the paper ""does not meet the standards"".149 To mitigate some deceptions, OpenAI developed a tool in 2024 to detect images that were generated by DALL-E 3.150In testing, this tool accurately identified DALL-E 3-generated images approximately 98 of the time. The tool is also fairly capable of recognizing images that have been visually modified by users post-generation.151 As generative AI image software such asStable DiffusionandDALL-Econtinue to advance, the potential problems and concerns that these systems pose for creativity and artistry have risen.129In 2022, artists working in various media raised concerns about the impact thatgenerative artificial intelligencecould have on their ability to earn money, particularly if AI-based images started replacing artists working in theillustration and design industries.152153In August 2022, digital artist R. J. Palmer stated that ""I could easily envision a scenario where using AI, a single artist or art director could take the place of 510 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they dont have to hire an artist.""119Scholars Jiang et al. state that ""Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently.""129A 2022 case study found that AI-produced images created by technology likeDALL-Ecaused some traditional artists to be concerned about losing work, while others use it to their advantage and view it as a tool.136 AI-based images have become more commonplace in art markets and search engines because AI-basedtext-to-image systemsare trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles.129154For example, Polish digital artist Greg Rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style.65Furthermore, some training databases on which AI systems are based are not accessible to the public. The ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed.129155156Works of AI-generated art, such asTh√©√¢tre D'op√©ra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at theColorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists.129155156TheNetflixshort filmThe Dog  the Boy, released in January 2023, received backlash online for its use of artificial intelligence art to create the film's background artwork.157Within the same vein,DisneyreleasedSecret Invasion, aMarvelTV show with an AI-generated intro, on Disney in 2023, causing concern and backlash regarding the idea that artists could be made obsolete by machine-learning tools.158 AI art has sometimes been deemed to be able to replace traditionalstock images.159In 2023,Shutterstockannounced a beta test of an AI tool that can regenerate partial content of other Shutterstock's images.Getty ImagesandNvidiahave partnered with the launch of Generative AI byiStock, a model trained on Getty's library and iStock's photo library using Nvidia's Picasso model.160 Researchers fromHugging FaceandCarnegie Mellon Universityreported in a 2023 paper that generating one thousand 10241024 images usingStable Diffusion's XL 1.0 base model requires 11.49kWhof energy and generates 1,594 grams 56.2 oz ofcarbon dioxide, which is roughly equivalent to driving an average gas-powered car a distance of 4.1 miles 6.6 km. Comparing 88 different models, the paper concluded that image-generation models used on average around 2.9kWh of energy per 1,000inferences.161 In addition to the creation of original art, research methods that use AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. According to CETINIC and SHE 2022, using artificial intelligence to analyze already-existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences.162163 Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.164Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification,object detection,multimodal tasks, knowledge discovery in art history, and computational aesthetics.163Synthetic images can also be used to train AI algorithms forart authenticationand to detect forgeries.165 Researchers have also introduced models that predict emotional responses to art. One such model is ArtEmis, a large-scale dataset paired with machine learning models. ArtEmis includes emotional annotations from over 6,500 participants along with textual explanations. By analyzing both visual inputs and the accompanying text descriptions from this dataset, ArtEmis enables the generation of nuanced emotional predictions.166167 AI has also been used in arts outside of visual arts. Generative AI has been used in video game productionbeyond imagery, especially forlevel designe.g., forcustom maps and creating new content e.g., quests or dialogue orinteractive storiesin video games.168169AI has also been used in theliterary arts,170such as helping withwriter's block, inspiration, or rewriting segments.171172173174In the culinary arts, some prototypecooking robotscan dynamicallytaste, which can assist chefs in analyzing the content and flavor of dishes during the cooking process.175 TITLE: Artificial intelligence art - Wikipedia",2025-05-06 13:42:32
https://en.wikipedia.org/wiki/Artificial_intelligence_in_government,Artificial intelligence in government - Wikipedia,"Artificial intelligence in government Contents Uses of AI in government Potential benefits Risks See also References Further reading Contributing to public policy objectives Assisting public interactions with government Gerrymandering Other uses Artificial intelligenceAI has a range of uses ingovernment. It can be used to furtherpublic policyobjectives in areas such as emergency services, health and welfare, as well as assist the public to interact with the government through the use ofvirtual assistants, for example. According to theHarvard Business Review, ""Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.""1Hila Mehr from theAsh Center for Democratic Governance and InnovationatHarvard Universitynotes that AI in government is not new, with postal services using machine methods in the late 1990s torecognise handwritingon envelopes to automatically route letters.2The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings for instance by reducing the number of front office staff, and reducing the opportunities for corruption.3However, it also carries risks described below. The potential uses of AI in government are wide and varied,4withDeloitteconsidering that ""Cognitive technologies could eventually revolutionize every facet of government operations"".5Mehr suggests that six types of government problems are appropriate for AI applications:2 Mehr states that ""While applications of AI in government work have not kept pace with the rapid expansion of AI in the private sector, the potential use cases in the public sector mirror common applications in the private sector.""2 Potential and actual uses of AI in government can be divided into three broad categories: those that contribute to public policy objectives; those that assist public interactions with the government; and other uses. There are a range of examples of where AI can contribute to public policy objectives.4These include: AI can be used to assist members of the public to interact with government and access government services,4for example by: Various governments, including those of Australia10and Estonia,11have implemented virtual assistants to aid citizens in navigating services, with applications ranging from tax inquiries to life-event registrations. Gerrymanderingis an insidious method of influencing political process.12Depending on the objective of its use, the application of artificial intelligence to redraw districts based on voter distribution and demographic datasets can either contribute to impartiality, or sustain partisan gains for interested stakeholders in the election process.13 Other uses of AI in government include: AI offers potential efficiencies and costs savings for the government. For example,Deloittehas estimated that automation could saveUS Governmentemployees between 96.7 million to 1.2 billion hours a year, resulting in potential savings of between 3.3 billion to 41.1 billion a year.5TheHarvard Business Reviewhas stated that while this may lead a government to reduce employee numbers, ""Governments could instead choose to invest in the quality of its services. They can re-employ workers' time towards more rewarding work that requires lateral thinking, empathy, and creativity  all things at which humans continue to outperform even the most sophisticated AI program.""1 Risks associated with the use of AI in government include AI becoming susceptible to bias,2a lack of transparency in how an AI application may make decisions,7and the accountability for any such decisions.7 AI in governance and the economic world might make the market more difficult for companies to keep up with the increases in technology. Large U.S. companies like Apple and Google are able to dominate the market with their latest and most advanced technologies. This gives them an advantage over smaller companies that do not have the means of advancing as far in the digital technology fields with AI.14 TITLE: Artificial intelligence in government - Wikipedia",2025-05-06 13:42:35
https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare,Artificial intelligence in healthcare - Wikipedia,"Artificial intelligence in healthcare Contents Applications in healthcare systems Clinical applications Industry Expanding care to developing nations Regulation Ethical concerns History See also References Further reading Disease diagnosis Electronic health records Drug interactions Telemedicine Workload management Cardiovascular Dermatology Gastroenterology Obstetrics and gynaecology Infectious diseases Musculoskeletal Neurology Oncology Ophthalmology Pathology Primary care Psychiatry Radiology Pharmacy United Nations WHOITU US FDA Data collection Automation Bias Artificial intelligence in healthcareis theapplication of artificial intelligenceAI to analyze and understand complex medical and healthcare data. In some cases, it can exceed or augment human capabilities by providing better or faster ways to diagnose, treat, or prevent disease.123 As the widespread use of AI in healthcare is still relatively new, research is ongoing into its applications across various medical subdisciplines and related industries. AI programs are being applied to practices such asdiagnostics,4treatment protocoldevelopment,5drug development,6personalized medicine,7andpatient monitoringand care.8Sinceradiographsare the most commonly performed imaging tests in radiology, the potential for AI to assist with triage and interpretation of radiographs is particularly significant.9 Using AI also presents unprecedented ethical concerns related to issues such asdata privacy, automation of jobs, and amplifying already existingbiases.10Furthermore, new technologies such as AI are often resisted by healthcare leaders, leading to slow and erratic adoption.11In contrast, there are also several cases where AI has been put to use in healthcare without proper testing.12131415A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.16Moreover, meta-studies have found that the scientific literature on AI in healthcare often suffers from a lack ofreproducibility.17181920 Accurate and early diagnosis of diseases is still a challenge in healthcare. Recognizing medical conditions and their symptoms is a complex problem. AI can assist clinicians with its data processing capabilities to save time and improve accuracy.21Through the use of machine learning, artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analysis of masselectronic health recordsEHRs.22AI can help early prediction, for example, ofAlzheimer's diseaseanddementias, by looking through large numbers of similar cases and possible treatments.23 Doctors' decision making could also be supported by AI in urgent situations, for example in theemergency department. Here AI algorithms can help prioritize more serious cases and reduce waiting time.Decision support systemsaugmented with AI can offer real-time suggestions and faster data interpretation to aid the decisions made by healthcare professionals.21 In 2023 a study reported higher satisfaction rates withChatGPT-generated responses compared with those from physicians for medical questions posted onReddits rAskDocs.24Evaluators preferred ChatGPT's responses to physician responses in 78.6 of 585 evaluations, noting better quality and empathy. The authors noted that these were isolated questions taken from an online forum, not in the context of an established patient-physician relationship.24Moreover, responses were not graded on the accuracy of medical information, and some have argued that the experiment was not properlyblinded, with the evaluators being coauthors of the study.252627 Recent developments instatistical physics,machine learning, andinferencealgorithms are also being explored for their potential in improving medical diagnostic approaches.28Also, the establishment of largehealthcare-related data warehousesof sometimes hundreds of millions of patients provides extensive training data for AI models.29 Electronic health records EHR are crucial to the digitalization and information spread of the healthcare industry. Now that around 80 of medical practices use EHR, some anticipate the use of artificial intelligence to interpret the records and provide new information to physicians.30 One application uses natural language processing NLP to make more succinct reports that limit the variation between medical terms by matching similar medical terms.30For example, the term heart attack andmyocardial infarctionmean the same things, but physicians may use one over the other based on personal preferences.30NLP algorithms consolidate these differences so that larger datasets can be analyzed.30Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read.30Other applications useconcept processingto analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.31 Beyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record andpredict a riskfor a disease based on their previous information and family history.32One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts.33This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses.33Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease.33Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time.32One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 7072 accuracy in predicting individualized treatment response.34These methods are helpful due to the fact that the amount of online health records doubles every five years.32Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.32 Improvements innatural language processingled to the development of algorithms to identifydrug-drug interactionsin medical literature.35363738Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken.39To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers atCarlos III Universityassembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms.40Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.41Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.353638 Other algorithms identify drug-drug interactions from patterns inuser-generated content, especially electronic health records andor adverse event reports.3637Organizations such as theFDA Adverse Event Reporting SystemFAERS and the World Health Organization'sVigiBaseallow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.42 The increase oftelemedicine, the treatment of patients remotely, has shown the rise of possible AI applications.43AI can assist in caring for patients remotely by monitoring their information through sensors.44A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.44 Another application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance onchatbots for mental healthcaredoes not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider be it a chat-bot or psychologist, though.45Some examples of these chatbots include Woebot, Earkick and Wysa.464748 Since the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations.49Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal.49Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.49 AI has the potential to streamline care coordination and reduce the workload. AI algorithms can automate administrative tasks, prioritize patient needs and facilitate seamless communication in a healthcare team.50This enables healthcare providers to focus more on direct patient care and ensures the efficient and coordinated delivery of healthcare services. Artificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool.5152Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment foracute coronary syndrome.51Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital.53A research in 2019 found that AI can be used to predict heart attack with up to 90 accuracy.54Another growing area of research is the utility of AI in classifyingheart soundsand diagnosingvalvular disease.55Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data onsocial determinants of healthas they pertain tocardiovascular disease.56 A key limitation in early studies evaluating AI were omissions of data comparing algorithmic performance to humans. Examples of studies which assess AI performance relative to physicians includes how AI is non-inferior to humans in interpretation of cardiac echocardiograms57and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.58 In cardiovasculartissue engineeringandorganoidstudies, AI is increasingly used to analyze microscopy images, and integrate electrophysiological read outs.59 Medical imagingsuch as X-ray and photography is a commonly used tool indermatology60and thedevelopment of deep learninghas been strongly tied toimage processing. Therefore, there is a natural fit between the dermatology and deep learning. Machine learning learning holds great potential to process these images for better diagnoses.61Han et al. showed keratinocytic skin cancer detection from face photographs.62Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images.63Noyan et al. demonstrated aconvolutional neural networkthat achieved 94 accuracy at identifying skin cells from microscopicTzanck smearimages.64A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.65 According to some researchers, AI algorithms have been shown to be more effective than dermatologists at identifying cancer.66However, a 2021 review article found that a majority of papers analyzing the performance of AI algorithms designed for skin cancer classification failed to use external test sets.67Only four research studies were found in which the AI algorithms were tested on clinics, regions, or populations distinct from those it was trained on, and in each of those four studies, the performance of dermatologists was found to be on par with that of the algorithm. Moreover, only one study68was set in the context of a full clinical examination; others were based on interaction through web-apps or online questionnaires, with most based entirely on context-free images of lesions. In this study, it was found that dermatologists significantly outperformed the algorithms. Many articles claiming superior performance of AI algorithms also fail to distinguish between trainees and board-certified dermatologists in their analyses.67 It has also been suggested that AI could be used to automatically evaluate the outcome ofmaxillo-facial surgeryorcleft palatetherapy in regard to facial attractiveness or age appearance.6970 AI can play a role in various facets of the field ofgastroenterology.Endoscopicexams such asesophagogastroduodenoscopiesEGD andcolonoscopiesrely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of earlystomach cancerhave shownsensitivityclose to expert endoscopists.71 AI can assist doctors treatingulcerative colitisin detecting the microscopic activity of the disease in people and predicting when flare-ups will happen. For example, an AI-powered tool was developed to analyse digitised bowel samples biopsies. The tool was able to distinguish with 80 accuracy between samples that showremissionof colitis and those with active disease. It also predicted the risk of a flare-up happening with the same accuracy. These rates of successfully using microscopic disease activity to predict disease flare are similar to the accuracy ofpathologists.7273 Artificial intelligence utilises massive amounts of data to help with predicting illness, prevention, and diagnosis, as well as patient monitoring. In obstetrics, artificial intelligence is utilized in magnetic resonance imaging, ultrasound, and foetal cardiotocography. AI contributes in the resolution of a variety of obstetrical diagnostic issues.74 AI has shown potential in both the laboratory and clinical spheres ofinfectious diseasemedicine.75During theCOVID-19 pandemic, AI has been used for early detection, tracking virus spread and analysing virus behaviour, among other things.76However, there were only a few examples of AI being used directly in clinical practice during the pandemic itself.77 Other applications of AI around infectious diseases includesupport-vector machinesidentifyingantimicrobial resistance, machine learning analysis of blood smears to detectmalaria, and improved point-of-care testing ofLyme diseasebased on antigen detection. Additionally, AI has been investigated for improving diagnosis ofmeningitis,sepsis, andtuberculosis, as well as predicting treatment complications inhepatitis Bandhepatitis Cpatients.75 AI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients.78Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.79 The use of AI technologies has been explored for use in the diagnosis and prognosis ofAlzheimer's diseaseAD. For diagnostic purposes, machine learning models have been developed that rely on structural MRI inputs.80The input datasets for these models are drawn from databases such as the Alzheimer's Disease Neuroimaging Initiative.81Researchers have developed models that rely onconvolutional neural networkswith the aim of improving early diagnostic accuracy.82Generative adversarial networksare a form ofdeep learningthat have also performed well in diagnosing AD.83There have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with AD. Forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets.84They suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases. AI has been explored for use incancerdiagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics.85AI has been trialed in cancer diagnostics with the reading of imaging studies andpathologyslides.86 In January 2020,Google DeepMindannounced an algorithm capable of surpassing human experts inbreast cancer detectionin screening scans.8788A number of researchers, includingTrevor Hastie,Joelle Pineau, andRobert Tibshiraniamong others, published a reply claiming that DeepMind's research publication inNaturelacked key details on methodology and code, ""effectively undermining its scientific value"" and making it impossible for the scientific community to confirm the work.89In theMIT Technology Review, author Benjamin Haibe-Kains characterized DeepMind's work as ""an advertisement"" having little to do with science.90 In July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date inidentifyingprostate cancer, with 98 sensitivity and 97 specificity.9192In 2023 a study reported the use of AI forCT-basedradiomicsclassification at grading the aggressiveness of retroperitonealsarcomawith 82 accuracy compared with 44 for lab analysis of biopsies.9394 Artificial intelligence-enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness.95In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm.96Moreover, AI technology may be used to further improve ""diagnosis rates"" because of the potential to decrease detection time.97 For many diseases,pathologicalanalysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods ofdigital pathologyallows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B,gastric cancer, andcolorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes.71AI is well-suited for use in low-complexity pathological analysis of large-scalescreeningsamples, such as colorectal orbreast cancerscreening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis.99Several deep learning and artificialneural networkmodels have shown accuracy similar to that of human pathologists,99and a study of deep learning assistance in diagnosingmetastaticbreast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone.100Additionally, implementation of digital pathology is predicted to save over 12 million for a university center over the course of five years,101though savings attributed to AI specifically have not yet been widely researched. The use ofaugmentedandvirtual realitycould prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review.99AI also has the potential to identifyhistologicalfindings at levels beyond what the human eye can see,99and has shown the ability to usegenotypicandphenotypicdata to more accurately detect the tumor of origin for metastatic cancer.102One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlledtrialsin determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.99 Primary care has become one key development area for AI technologies.103104AI in primary care has been used for supporting decision making, predictive modeling, and business analytics.105There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.106 In psychiatry, AI applications are still in a phase of proof-of-concept.107Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes,108chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.109 Challenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017.110Such applications outside the healthcare system raise various professional, ethical and regulatory questions.111Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.112 In 2023, US-basedNational Eating Disorders Associationreplaced its humanhelplinestaff with achatbotbut had to take it offline after users reported receiving harmful advice from it.113114115 AI is being studied within the field ofradiologyto detect and diagnose diseases throughcomputerized tomographyCT andmagnetic resonanceMR imaging.116It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers.117Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated.118AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality,119and automatically assessing image quality.120Further research investigating the use of AI innuclear medicinefocuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies.121The analysis of images for supervised AI applications in radiology encompasses two primary techniques at present: 1convolutional neural network-basedanalysis; and 2 utilization ofradiomics.117 AI is also used in breast imaging for analyzing screening mammograms and can participate in improving breast cancer detection rate122as well as reducing radiologist's reading workload. The trend of large health companies merging allows for greater health data accessibility. Greater health data lays the groundwork for the implementation of AI algorithms. A large part of industry focus of implementation of AI in the healthcare sector is in theclinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions.116Numerous companies are exploring the possibilities of the incorporation ofbig datain the healthcare industry. Many companies investigate the market opportunities through the realms of ""data assessment, storage, management, and analysis technologies"" which are all crucial parts of the healthcare industry.130 The following are examples of large companies that have contributed to AI algorithms for use in healthcare: Digital consultant apps use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solutionbuzzword to the marketplace. These archetypes depend on the value generated for the target user e.g. patient focus vs. healthcare provider and payer focus and value capturing mechanisms e.g. providing information or connecting stakeholders. IFlyteklaunched a service robot ""Xiao Man"", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH ""Cruzr"" andSoftbankRobotics ""Pepper"". The Indian startupHaptikrecently developed aWhatsAppchatbot which answers questions associated with the deadlycoronavirusinIndia. Similarly, a software platformChatBotin partnership withmedtechstartupInfermedica launchedCOVID-19Risk Assessment ChatBot.133 With the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies.130Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well.130Companies such asBMW,GE,Tesla,Toyota, andVolvoall have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances.130 Artificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public. Many new technology companies such asSpaceXand theRaspberry Pi Foundationhave enabled more developing countries to have access to computers and the internet than ever before.134With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.134 Using AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient.135The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.135 While research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may nonetheless introduce several new types of risk to patients and healthcare providers, such asalgorithmic bias,Do not resuscitateimplications, and othermachine moralityissues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection.136These challenges of the clinical use of AI have brought about a potential need forregulations. AI studies need to be completely and transparently reported to have value to inform regulatory approval. Depending on the phase of study, international consensus-based reporting guidelines TRIPODAI,137DECIDE-AI,138CONSORT-AI139 have been developed to provide recommendations on the key details that need to be reported. Currently, there are regulations pertaining to the collection of patient data. This includes policies such as the Health Insurance Portability and Accountability Act HIPAA and the European General Data Protection Regulation GDPR.140The GDPR pertains to patients within the EU and details the consent requirements for patient data use when entities collect patient healthcare data. Similarly, HIPAA protects healthcare data from patient records in the United States.140In May 2016, theWhite Houseannounced its plan to host a series of workshops and formation of theNational Science and Technology CouncilNSTC Subcommittee on Machine Learning and Artificial Intelligence. In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development within government and academia. The report notes a strategic RD plan for the subfield ofhealth information technologyis in development stages. There is concern that large language models can overwhelm people with both accurate health information and also misinformation, leading to potential challenges in public health. This calls for the need for policy and user guidance related to health information through AI.141 The jointITU-WHOFocus Group on Artificial Intelligence for HealthFG-AI4H has built a platform - known as the ITU-WHOAI for Health Framework- for the testing and benchmarking of AI applications in health domain. As of November 2018, eight use cases are being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions. In January 2021, theUSFDApublished a new Action Plan, entitled Artificial Intelligence AI Machine Learning ML-Based Software as a Medical Device SaMD Action Plan.143This plan lays out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software. There are five main actions the FDA plans to take to increase regulation: 1. Tailored Regulatory Framework for AiM:-based SaMD, 2. Good Machine Learning Practice GMLP, 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias  Robustness, and 5. Real-World PerformanceRWP. This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA. According to the U.S. Department of Health and Human Services, the Office for Civil Rights OCR has issued guidance on theethical use of AIin healthcare. The guidance outlines four core ethical principles that must be followed: respect for autonomy, beneficence, non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. Moreover, as of March 2021, the OCR hired a Chief Artificial Intelligence Officer OCAIO to pursue the ""implementation of the HHS AI strategy"".144The OCR also has issued rules and regulations to protect the privacy of individuals health information. These regulations require healthcare providers to follow certain privacy rules when using AI. The OCR also requires healthcare providers to keep a record of how they use AI and to ensure that their AI systems are secure. Overall, the U.S. has taken steps to protect individuals privacy and ethical issues related to AI in healthcare145 The U.S. is not the only country to develop or initiate regulations of data privacy with AI. Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on Data Ethics has adopted recommendations on 'Data for the Benefit of the People'. These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity. The importance of human dignity is stressed, as it is said to outweigh profit and must be respected in all data processes146 The European Union has implemented the General Data Protection Regulation GDPR to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency.147With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy.147In the United States, the Health Insurance Portability and Accountability Act HIPAA requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.148 In order to effectively train Machine Learning and use AI in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy in most cases and is not well received publicly. For example, a survey conducted in the UK estimated that 63 of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology.140The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare. Furthermore, the lack of current regulations surrounding AI in the United States has generated concerns about mismanagement of patient data, such as with corporations utilizing patient data for financial gain. For example,Roche, a Swiss healthcare company, was found to have purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of 1.9 billion.149Naturally, this generates questions of ethical concern; Is there a monetary price that can be set for data, and should it depend on its perceived value or contributions to science? Is it fair to patients to sell their data? These concerns were addressed in a survey conducted by thePew Research Centerin 2022 that asked Americans for their opinions about the increased presence of AI in their daily lives, and the survey estimated that 37 of Americans were more concerned than excited about such increased presence, with 8 of participants specifically associating their concern with ""people misusing AI"".150Ultimately, the current potential of artificial intelligence in healthcare is additionally hindered by concerns about mismanagement of data collected, especially in the United States. A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.16 According to a 2019 study, AI can replace up to 35 of jobs in the UK within the next 10 to 20 years.151However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.151 Automation can provide benefits alongside doctors as well. Some believe that AI may avert healthcare worker burnout and cognitive overload, so that doctors who take advantage of AI in healthcare will provide greater quality healthcare than doctors and medical establishments who do not.152 Recently, there have been many discussions between healthcare experts in terms of AI and elder care. In relation to elder care, AI bots have been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand153 Since AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care.154A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed usingmany-to-manymapping.155 There can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities.154Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. For instance, if populations are less represented in healthcare data it is likely to create bias in AI tools that lead to incorrect assumptions of a demographic and impact the ability to provide appropriate care.156White males are overly represented in medical data sets.157Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations.158Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients.157In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes e.g., resolution vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult.159However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data. A final source ofalgorithmic bias, which has been called ""label choice bias"", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients.160Solutions to the ""label choice bias"" aim to match the actual target what the algorithm is predicting more closely to the ideal target what researchers want the algorithm to predict, so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program. Research in the 1960s and 1970s produced the first problem-solving program, orexpert system, known asDendral.161162While it was designed for applications in organic chemistry, it provided the basis for a subsequent systemMYCIN,163considered one of the most significant early uses of artificial intelligence in medicine.163164MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.165 The 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians.166Approaches involvingfuzzy settheory,167Bayesian networks,168andartificial neural networks,169170have been applied to intelligent computing systems in healthcare. Medical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include: TITLE: Artificial intelligence in healthcare - Wikipedia",2025-05-06 13:42:39
https://en.wikipedia.org/wiki/Artificial_intelligence_in_mental_health,Artificial intelligence in mental health - Wikipedia,"Artificial intelligence in mental health Contents Background AI-driven approaches Applications Benefits Challenges Current AI trends in mental health Outcome Comparisons: AI vs Traditional Therapy Criticism Ethical issues Addressing Bias and Discrimination in AI-Based Mental Health Tools Prospects and Conclusion: See also References Further reading Machine learning Natural language processing Deep Learning Computer Vision LLMs and Gen AI Diagnosis Prognosis Treatment Artificial intelligence in mental healthrefers to theapplication of artificial intelligenceAI, computational technologies andalgorithmsto support the understanding, diagnosis, and treatment ofmental health disorders.123In the context ofmental health, AI is considered a component of digital healthcare, with the objective of improving accessibility and accuracy and addressing the growing prevalence of mental health concerns.4Applications of AI in this field include the identification and diagnosis of mental disorders, analysis of electronic health records, development of personalized treatment plans, andanalyticsfor suicide prevention.45There is also research into, and private companies offering,AI therapistswhich providetalk therapiessuch as cognitive behavioural therapy. Despite its many potential benefits, the implementation of AI in mental healthcare presents significant challenges and ethical considerations, and its adoption remains limited as researchers and practitioners work to address existing barriers.4 Artificial Intelligence is a rapidly booming field with successful advancements in the field of healthcare. It worked its way into mental health starting major developments in diagnosis, prognosis and treatments. Implementing AI in mental health can eliminate the stigma and seriousness of mental health issues globally. The recent grasp on mental health issues has brought out concerning facts like depression, affecting millions of people annually. The current application of AI in mental health does not meet the demand to mitigate global mental health concerns. In this article, ethical concerns such as data privacy and unlawful access to sensitive information will be addressed. The question of whether chatbots are sentient enough to be used as mental health counsellors is also discussed in this paper.6 In 2019, 1 in every 8 people, or 970 million people around the world were living with a mental disorder, withanxietyanddepressive disordersbeing the most common.7In 2020, the number of people living with anxiety and depressive disorders rose significantly because of theCOVID-19 pandemic.8Additionally, the prevalence of mental health and addiction disorders exhibits a nearly equal distribution across genders, emphasizing the widespread nature of the issue.9 The use of AI in mental health aims to support responsive and sustainable interventions against the global challenge posed by mental health disorders. Some issues common to the mental health industry are provider shortages, inefficient diagnoses, and ineffective treatments. The Global market for AI-driven mental health applications is projected to grow significantly, with estimates suggesting an increase from 0.92 billion USD in 2023 to 14.89 billion USD by 2033.10This growth indicates a growing interest in AI's ability to address critical challenges in mental healthcare provision through the development and implementation of innovative solutions.11 Several AI technologies, includingmachine learningML,natural language processingNLP,deep learningDL,computer visionCV and LLMs and Gen AI are currently applied in various mental health contexts. These technologies enable early detection of mental health conditions, personalized treatment recommendations, and real-time monitoring of patient well-being. Machine learning is an AI technique that enables computers to identify patterns in large datasets and make predictions based on those patterns. Unlike traditional medical research, which begins with a hypothesis, ML models analyze existing data to uncover correlations and develop predictive algorithms.11ML in psychiatry is limited by data availability and quality. Many psychiatric diagnoses rely on subjective assessments, interviews, and behavioral observations, making structured data collection difficult.11Some researchers have appliedtransfer learning, a technique that adapts ML models trained in other fields, to overcome these challenges in mental health applications.12 Natural Language Processing allows AI systems to analyze and interpret human language, including speech, text, and tone of voice. In mental health, NLP is used to extract meaningful insights from conversations, clinical notes, and patient-reported symptoms. NLP can assess sentiment, speech patterns, and linguistic cues to detect signs of mental distress. This is crucial because many of the diagnoses andDSM-5mental health disorders are diagnosed via speech in doctor-patient interviews, utilizing the clinician's skill for behavioral pattern recognition and translating it into medically relevant information to be documented and used for diagnoses. As research continues, NLP models must address ethical concerns related to patient privacy, consent, and potential biases in language interpretation.13 Advancements in NLP such as sentiment analysis identifies distinctions in tone and speech to detect anxiety and depression. Woebot, uses sentiment analysis to scrutinize and detect patterns for depression or despair and suggests professional help to patients. Similarly, Cogito, an AI platform uses voice analysis to find changes in pitch and loudness to identify symptoms of depression or anxiety. The application of NLP can contribute to early diagnosis and improved treatment strategies.1415 Deep learning, a subset of ML, involves neural networks that mimic the human brain to analyze complex data. It is particularly useful for identifying subtle patterns in speech, imaging, and physiological data.16Deep learning techniques have been applied in neuroimaging research to identify abnormalities in brain scans associated with conditions such as schizophrenia, depression, and PTSD.17However, deep learning models require extensive, high-quality datasets to function effectively. The limited availability of large, diverse mental health datasets poses a challenge, as patient privacy regulations restrict access to medical records. Additionally, deep learning models often operate as ""black boxes"", meaning their decision-making processes are not easily interpretable by clinicians, raising concerns about transparency and clinical trust.18 Computer vision enables AI to analyze visual data, such as facial expressions, body language, and micro expressions, to assess emotional and psychological states. This technology is increasingly used in mental health research to detect signs of depression, anxiety, and PTSD through facial analysis.19Computer vision tools have been explored for their ability to detect nonverbal cues, such as hesitation or changes in eye contact, which may correlate with emotional distress. Despite its potential, computer vision in mental health raises ethical and accuracy concerns. Facial recognition algorithms can be influenced byculturalandracial biases, leading to potential misinterpretations of emotional expressions.20Additionally, concerns about informed consent and data privacy must be addressed before widespread clinical adoption. From the introduction of LLMs in the field of AI in correlation to mental health care, a lot of developments have come about. Popular examples of LLMs are ChatGPT and Gemini. LLMs have been trained on a lot of data which has made it capable of being considerate and even mimic how a human behaves but chatbots are only fed scripted data which gives it the lack of empathy when dealing with patients. This kind of LLM technology is very useful for people who hesitate to ask for assistance or dont have access to get treatment.21 But at the same time, LLMs have not exactly been known to be as effective as they seem capable of being. LLMs can experience a condition called hallucination where they can possibly give wrong medical advice to the patients that can be extremely dangerous. LLMs do not exhibit the required level of compassion or empathy needed specially in difficult situations.21 AI with the use of NLP and ML can be used to help diagnose individuals with mental health disorders. It can be used to differentiate closely similar disorders based on their initial presentation to inform timely treatment before disease progression. For example, it may be able to differentiateunipolarfrombipolardepression by analyzing imaging and medical scans.11AI also has the potential to identify novel diseases that were overlooked due to the heterogeneity of presentation of a single disorder.11Doctors may overlook the presentation of a disorder because while many people get diagnosed with depression, that depression may take on different forms and be enacted in different behaviors. AI can parse through the variability found in human expression data and potentially identify different types of depression. AI can be used to create accurate predictions for disease progression once diagnosed.11AI algorithms can also use data-driven approaches to build new clinical risk prediction models22without relying primarily on current theories ofpsychopathology. However, internal and external validation of an AI algorithm is essential for its clinical utility.11In fact, some studies have usedneuroimaging, electronic health records, genetic data, and speech data to predict how depression would present in patients, their risk forsuicidalityorsubstance abuse, or functional outcomes.11The prognosis seems to be highly promising, though it comes with important challenges and ethical considerations such as: Early detention AI can analyze patterns in speech, writing, facial expressions, and social media behavior to detect early signs of depression, anxiety, PTSD, and even schizophrenia.The New Yorker Can A.I. Treat Mental Illness? In psychiatry, in many cases multiple drugs are trialed with the patients until the correct combination or regimen is reached to effectively treat their ailmentAI systems have been investigated for their potential to predict treatment response based on observed data collected from various sources. This application of AI has the potential to reduce the time, effort, and resources required while alleviating the burden on both patients and clinicians.11 Artificial intelligence offers several potential advantages in the field of mental health care: Despite its potential, the application of AI in mental health presents a number of ethical, practical, and technical challenges: As of 2020, theFood and Drug Administration FDAhad not yet approved any artificial intelligence-based tools for use inPsychiatry.27However, in 2022, the FDA granted authorization for the initial testing of an AI-driven mental health assessment tool known as the AI-Generated Clinical Outcome Assessment AI-COA. This system employs multimodal behavioral signal processing and machine learning to track mental health symptoms and assess the severity of anxiety and depression. AI-COA was incorporated into a pilot program to evaluate its clinical effectiveness. As of 2025, it has not received full regulatory approval.28 Mental health tech startups continue to lead investment activity in digital health despite the ongoing impacts of macroeconomic factors like inflation, supply chain disruptions, and interest rates.29 According to CB Insights,State of Mental Health Tech 2021 Report, mental health tech companies raised 5.5 billion worldwide 324 deals, a 139 increase from the previous year that recorded 258 deals. A number of startups that are using AI in mental healthcare have closed notable deals in 2022 as well. Among them is the AI chatbotWysa20 million in funding,BlueSkeyethat is working on improving early diagnosis 3.4 million, theUphealsmart notebook for mental health professionals 1.068 million, and the AI-based mental health companionclareme1 million.30Founded in 2021, Earkick serves as an 'AI therapist' for mental health support.3132 An analysis of the investment landscape and ongoing research suggests that we are likely to see the emergence of more emotionally intelligent AI bots and new mental health applications driven by AI prediction and detection capabilities. For instance, researchers atVanderbilt University Medical Centerin Tennessee, US, have developed an ML algorithm that uses a persons hospital admission data, including age, gender, and past medical diagnoses, to make an 80 accurate prediction of whether this individual is likely to take their own life.33And researchers at theUniversity of Floridaare about to test their new AI platform aimed at making an accurate diagnosis in patients with early Parkinsons disease.34Research is also underway to develop a tool combiningexplainable AIanddeep learningto prescribe personalized treatment plans for children with schizophrenia.35 AI systems could predict and plan treatments accurately and effectively for all fields of medicine at levels similar to that of physicians and general clinical practices. For example, one AI model demonstrated higher diagnostic accuracy for depression and post-traumatic stress disorder compared to general practitioners in controlled studies.36 AI systems that analyze social media data are being developed to detect mental health risks more efficiently and cost-effectively across broader populations. Ethical concerns include uneven performance between digital services, the possibility that biases could affect decision-making, and trust, privacy, and doctor-patient relationship issues.36 In January 2024, Cedars-Sinai physician-scientists developed a first-of-its-kind program that uses immersive virtual reality and generative artificial intelligence to provide mental health support.2The program is called XAIA which employs a large language model programmed to resemble a human therapist.3 The University of Southern California is researching the effectiveness of a virtual therapist named Ellie. Through a webcam and microphone, this AI is able to process and analyze the emotional cues derived from the patient's face and the variation in expressions and tone of voice.4 A team of Stanford Psychologists and AI experts created ""Woebot"". Woebot is an app that makes therapy sessions available 247. WoeBot tracks its users' mood through brief daily chat conversations and offers curated videos or word games to assist users in managing their mental health.5A Scandinavian team of software engineers and a clinical psychologist created ""Heartfelt Services"". Heartfelt Services is an application meant to simulate conventional talk therapy with an AI therapist.37 Incorporating AI with EHR records, genomic data and clinical prescriptions can contribute to precision treatment. Oura Ring, a wearable technology scans the individuals heart rate and sleep routine in real time to give tailored suggestions. Such AI-based application has an increasing potential in combating the stigma of mental health.3839 Research shows that AI-driven mental health tools, particularly those usingcognitive behavioral therapyCBT, can improve symptoms of anxiety and depression, especially for mild to moderate cases. For example, chatbot-based interventions like Woebot significantly reduced depressive symptoms in young adults within two weeks, with results comparable to brief human-delivered interventions.40A 2022 meta-analysis of digital mental health tools, including AI-enhanced apps, found moderate effectiveness in reducing symptoms when user engagement was high, and interventions were evidence-based.41 However, traditional therapy remains more effective for complex or high-risk mental health conditions that require emotional nuance and relational depth, such as PTSD, severe depression, or suicidality. The therapeutic alliance, or the relationship between patient and clinician, is frequently cited in clinical literature as a significant factor in treatment outcomes, accounting for up to 30 of positive outcomes.42While AI tools are capable of detecting patterns in behavior and speech, they are currently limited in replicating emotional nuance and the social context sensitivity typically provided by human clinicians. As such, most experts view AI in mental health as a complementary tool, best used for screening, monitoring, or augmenting care between human-led sessions.43 While AI systems excel at processing large datasets and providing consistent, round-the-clock support, their rigidity and limitations in contextual understanding remain significant barriers. Human therapists can adapt in real time to tone, body language, and life circumstancessomething machine learning models have yet to master.4143Nonetheless, integrated models that pair AI-driven symptom tracking with clinician oversight are showing promise. These hybrid approaches may increase access, reduce administrative burden, and support early detection, allowing human clinicians to focus on relational care. Current research suggests that AI in mental health care is more likely to augment rather than replace clinician-led therapy, particularly by supporting data analysis and continuous monitoring Although artificial intelligence in mental health is a growing field with significant potential, several concerns and criticisms remain regarding its application: Although significant progress is still required, the integration of AI in mental health underscores the need for legal and regulatory frameworks to guide its development and implementation.4Achieving a balance between human interaction and AI in healthcare is challenging, as there is a risk that increased automation may lead to a more mechanized approach, potentially diminishing the human touch that has traditionally characterized the field.5Furthermore, granting patients a feeling of security and safety is a priority considering AI's reliance on individual data to perform and respond to inputs. Some experts caution that efforts to increase accessibility through automation may unintentionally affect aspects of the patient experience, such as trust or perceived support.5To avoid veering in the wrong direction, more research should continue to develop a deeper understanding of where the incorporation of AI produces advantages and disadvantages.24 Data privacy and confidentiality are one of the most common security threats to medical data. Chatbots are known to be used as virtual assistants for patients but the sensitive data they collect may not be protected because the US law does not consider them as medical devices. Pharmaceutical companies use this loophole to access sensitive information and use it for their own purpose which results, in a lack of trust in chatbots and patients can hesitate in providing information essential to their treatment. Conversational Artificial Intelligence stores and remembers every conversation with a patient with complete accuracy, smartphones also collect data from search history and track app activity. If such private information is leaked it could further increase the stigma around mental health. The danger of cybercrimes and the governments unprotected access to our data, all raise serious concerns about data security.5253 Additionally, a lack of clarity and openness with AI models can lead to a loss of trust from the patient for their medical advisors or doctors as the regular person is unaware of how they reach conclusions into giving certain medical advice. Access to such information is necessary to build trust. However, many of these models act like black boxes, providing very little insight into how they work. AI specialists have thus highlighted ethical standards, diverse data and the correct usage of AI tools in mental healthcare.54 Artificial intelligence has shown promise in transforming mental health care through tools that support diagnosis, symptom tracking, and personalized interventions. However, significant concerns remain about the ways these systems may inadvertently reinforce existing disparities in care. Because AI models rely heavily on training data, they are particularly vulnerable to bias if that data fails to reflect the full range of racial, cultural, gender, and socioeconomic diversity found in the general population. For example, a 2024 study from the University of California found that AI systems analyzing social media data to detect depression exhibited significantly reduced accuracy for Black Americans compared to white users, due to differences in language patterns and cultural expression that were not adequately represented in the training data.55Similarly, natural language processing NLP models used in mental health settings may misinterpret dialects or culturally specific forms of communication, leading to misdiagnoses or missed signs of distress. These kinds of errors can compound existing disparities, particularly for marginalized populations that already face reduced access to mental health services. Biases can also emerge during the design and deployment phases of AI development. Algorithms may inherit the implicit biases of their creators or reflect structural inequalities present in health systems and society at large. These issues have led to increased calls for fairness, transparency, and equity in the development of mental health technologies. In response, researchers and healthcare institutions are taking steps to address bias and promote more equitable outcomes. Key strategies include: These efforts are still in early stages, but they reflect a growing recognition that equity must be a foundational principle in the deployment of AI in mental health care. When designed thoughtfully, AI systems could eventually help reduce disparities in care by identifying underserved populations, tailoring interventions, and increasing access in remote or marginalized communities. Continued investment in ethical design, oversight, and participatory development will be essential to ensure that AI tools do not replicate historical injustices but instead help move mental health care toward greater equity. AI in mental health is progressing with personalized care to incorporate voice, speech and biometric data. But to prevent algorithmic bias, models need to be culturally inclusive too. In conclusion, the current article provides a strong point of AI in mental health but topics like ethical issues, practical uses and bias in generative models need to be addressed to ensure effective mental healthcare.5960 TITLE: Artificial intelligence in mental health - Wikipedia",2025-05-06 13:42:42
https://en.wikipedia.org/wiki/Artificial_intelligence_in_industry,Artificial intelligence in industry - Wikipedia,"Artificial intelligence in industry Contents Categories Challenges Standard processes for data science in production Industrial data sources Artificial intelligence for business education See also References Process and industry characteristics Data characteristics Machine learning model characteristics Academic programs Curriculum structure Accreditation Industrial artificial intelligence, orindustrial AI, usually refers to the application ofartificial intelligenceto industry and business. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis1and insight discovery.2 Artificial intelligence andmachine learninghave become key enablers to leverage data in production in recent years due to a number of different factors: More affordable sensors and the automated process of data acquisition; More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost; Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing.3 Possible applications of industrial AI and machine learning in the production domain can be divided into seven application areas:4 Each application area can be further divided into specific application scenarios that describe concrete AIML scenarios in production. While some application areas have a direct connection to production processes, others cover production adjacent fields like logistics or the factory building.4 An example from the application scenarioProcess Design  Innovationarecollaborative robots. Collaborative robotic arms are able to learn the motion and path demonstrated by human operators and perform the same task.5Predictiveandpreventive maintenancethrough data-drivenmachine learningare exemplary application scenarios from theMachinery  Equipmentapplication area.4 In contrast to entirely virtual systems, in which ML applications are already widespread today, real-world production processes are characterized by the interaction between the virtual and the physical world. Data is recorded using sensors and processed on computational entities and, if desired, actions and decisions are translated back into the physical world via actuators or by human operators.6This poses major challenges for the application of ML in production engineering systems. These challenges are attributable to the encounter of process, data and model characteristics: The production domain's high reliability requirements, high risk and loss potential, the multitude of heterogeneous data sources and the non-transparency of ML model functionality impede a faster adoption of ML in real-world production processes. In particular, production data comprises a variety of different modalities, semantics and quality.7Furthermore, production systems are dynamic, uncertain and complex,7and engineering and manufacturing problems are data-rich but information-sparse.8Besides that, due the variety of use cases and data characteristics, problem-specific data sets are required, which are difficult to acquire, hindering both practitioners and academic researchers in this domain.9 The domain of production engineering can be considered as a rather conservative industry when it comes to the adoption of advanced technology and their integration into existing processes. This is due to high demands on reliability of the production systems resulting from the potentially high economic harm of reduced process effectiveness due to e.g., additional unplanneddowntimeor insufficient product qualities. In addition, the specifics of machining equipment and products prevent area-wide adoptions across a variety of processes. Besides the technical reasons, the reluctant adoption of ML is fueled by a lack of IT and data science expertise across the domain.4 The data collected in production processes mainly stem from frequently sampling sensors to estimate the state of a product, a process, or the environment in the real world. Sensor readings are susceptible to noise and represent only an estimate of the reality under uncertainty. Production data typically comprises multiple distributed data sources resulting in various data modalities e.g., images from visual quality control systems, time-series sensor readings, or cross-sectional job and product information. The inconsistencies in data acquisition lead to lowsignal-to-noise ratios, low data quality and great effort in data integration, cleaning and management. In addition, as a result from mechanical and chemical wear of production equipment, process data is subject to various forms ofdata drifts. ML models are considered asblack-box systemsgiven their complexity and intransparency of input-output relation. This reduces the comprehensibility of the system behavior and thus also the acceptance by plant operators. Due to the lack of transparency and the stochasticity of these models, no deterministic proof of functional correctness can be achieved complicating the certification of production equipment. Given their inherent unrestricted prediction behavior, ML models are vulnerable against erroneous or manipulated data further risking the reliability of the production system because of lacking robustness and safety. In addition to high development and deployment costs, the data drifts cause high maintenance costs, which is disadvantageous compared to purelydeterministic programs. The development of ML applications  starting with the identification and selection of the use case and ending with the deployment and maintenance of the application  follows dedicated phases that can be organized in standard process models. The process models assist in structuring the development process and defining requirements that must be met in each phase to enter the next phase. The standard processes can be classified into generic and domain-specific ones. Generic standard processes e.g.,CRISP-DM, ASUM-DM,KDD,SEMMA, orTeam Data Science Process describe a generally valid methodology and are thus independent of individual domains.10Domain-specific processes on the other hand consider specific peculiarities and challenges of special application areas. TheMachine Learning Pipeline in Productionis a domain-specific data science methodology that is inspired by the CRISP-DM model and was specifically designed to be applied in fields of engineering and production technology.11To address the core challenges of ML in engineering  process, data, and model characteristics  the methodology especially focuses on use-case assessment, achieving a common data and process understanding data integration, data preprocessing of real-world production data and the deployment and certification of real-world ML applications. The foundation of most artificial intelligence and machine learning applications in industrial settings are comprehensive datasets from the respective fields. Those datasets act as the basis for training the employed models.7In other domains, like computer vision, speech recognition or language models, extensive reference datasets e.g.ImageNet, Librispeech,12The People's Speech and data scraped from the open internet13are frequently used for this purpose. Such datasets rarely exist in the industrial context because of high confidentiality requirements9and high specificity of the data. Industrial applications of artificial intelligence are therefore often faced with the problem of data availability.9 For these reasons, existing open datasets applicable to industrial applications, often originate from public institutions like governmental agencies or universities and data analysis competitions hosted by companies. In addition to this, data sharing platforms exist. However, most of these platforms have no industrial focus and offer limited filtering abilities regarding industrial data sources. Artificial intelligence for business educationrefers to the academic programs offered by universities that integrateartificial intelligenceAI with business management principles. These programs aim to prepare students for the increasing role of AI in business, equipping them with the skills necessary to apply AI technologies to areas such as predictive analytics, supply chain optimization, and decision-making.14AI for business education programs are offered at both undergraduate and graduate levels by several universities globally. Bachelor in Artificial Intelligence for Business BAIB, Bachelor in Computer Science and Artificial Intelligence BCSAI, Master of Science in Artificial Intelligence in Business MS-AIB  These are new programs that are still in their first cohorts and have yet to prove themselves in the industry. The undergraduate degrees are often offered in conjuction with a BBA as a 5-year double degree program, the undergraduate degrees are going through the acreditation processes in their respective countries. Programs that combine AI with business studies vary by institution and degree level. Below are some notable examples: The Bachelor in Artificial Intelligence for Business BAIB- This program, started byEsadefocuses on the integration of AI and machine learning with core business disciplines such as management, marketing, and finance. TheEsade Business Schoolis a highly regarded institution for its business innovation, sustainability focus and future-proof outlook. During the BBABAIB, students are trained to apply AI in business environments to improve efficiency, innovation, and decision-making.15 Bachelor in Computer Science and Artificial Intelligence BCSAI Offered along with a BBA byIE University, the BCSAI combines foundational studies in computer science with a specialization in artificial intelligence. The program also provides a strong grounding in business principles, preparing graduates to create AI solutions for business problems and drive technological innovation in the business world.16 Master in Artificial Intelligence for Business MS-AIBArizona State UniversityASU offers a graduate-level program focused on AI applications in business environments. This degree explores advanced topics such as AI-driven decision-making, big data analysis, and the ethical implications of AI in business. The program is designed for professionals seeking to leverage AI technologies to transform business practices and improve efficiency. These programs typically include a combination of AI and business courses. Core subjects often cover topics such as machine learning, data science, business strategy, and financial management. The programs aim to give students a broad understanding of AI applications within a business environment, while also allowing them to specialize in areas such as supply chain management, marketing analytics, and AI-driven innovation. In addition to technical courses, many programs include practical training, such as internships, real-world AI projects, and industry case studies. This helps students gain practical experience in applying AI tools and techniques to solve business challenges. Many universities offering these degrees hold accreditation from recognized educational bodies, ensuring that their programs meet rigorous academic and industry standards. For example,ESADEandIE Universityare both accredited by institutions such as EQUIS and AACSB, which evaluate the quality of business education programs. Similarly,Arizona State Universityholds accreditation for its graduate programs in business and technology. TITLE: Artificial intelligence in industry - Wikipedia",2025-05-06 13:42:45
https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race,Artificial intelligence arms race - Wikipedia,"Artificial intelligence arms race Contents Terminology History Risks By nation Proposals for international regulation Other reactions to autonomous weapons Disassociation Rankings See also References Further reading United States China India Russia Israel United Kingdom South Korea European Union Donald TrumpJoe BidenBarack ObamaElon MuskSundar PichaiJensen HuangSam AltmanSatya NadellaAndy JassyTim CookMark Zuckerberg Xi JinpingHu JintaoJiang ZeminJack MaRobin LiLiang WenfengPony MaDaniel ZhangRen ZhengfeiTan RuisongLei Jun United StatesGoogleNvidiaOpenAIMicrosoftAmazonAppleTeslaMetaIBMIntelTexas InstrumentsNational InstrumentsLockheed Martin ChinaBaiduDeepSeekTencentAlibabaHuaweiSenseTimeiFlytekAlphaXiaomiMegviiYMTCSilanAVIC Est. 300 billionUSA, over the last decade Est. 200 billion China, over the last decade AI regulation concernsData privacy issuesAI bias and fairness China's Data Security Law A militaryartificial intelligence arms raceis anarms racebetween two or more states to develop and deploylethal autonomous weaponssystems LAWS. Since the mid-2010s, many analysts have noted the emergence of such an arms race betweensuperpowersfor bettermilitary AI,12driven byincreasing geopolitical and military tensions. An AI arms race is sometimes placed in the context of anAI Cold Warbetween theUnited StatesandChina.3 Lethal autonomous weaponssystems use artificial intelligence to identify and kill human targets without human intervention.4LAWS have colloquially been called ""slaughterbots"" or ""killer robots"". Broadly, any competition for superior AI is sometimes framed as an ""arms race"".56Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages.7 In 2014, AI specialistSteve Omohundrowarned that ""An autonomous weapons arms race is already taking place"".9According toSiemens, worldwide military spending on robotics was US5.1 billion in 2010 and US7.5 billion in 2015.1011 China became a top player in artificial intelligence research in the 2010s. According to theFinancial Times, in 2016, for the first time, China published more AI research papers than the entire European Union. When restricted to number of AI papers in the top 5 of cited papers, China overtook the United States in 2016 but lagged behind the European Union.1223 of the researchers presenting at the 2017American Association for the Advancement of Artificial IntelligenceAAAI conference were Chinese.8Eric Schmidt, the former chairman ofAlphabet, has predicted China will be the leading country in AI by 2025.13 One risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, increasing the risk of critical failures and unintended consequences.1415This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using ""race"" terminology at all in this context can exacerbate this effect.16 Another potential danger of an AI arms race is the possibility of losing control of the AI systems; the risk is compounded in the case of a race toartificial general intelligence, which may present anexistential risk.16In 2023, aUnited States Air Forceofficial reportedly said that during acomputer test, a simulated AI drone killed the human character operating it. The USAF later said the official had misspoken and that it never conducted such simulations.17 A third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group.16A US government report argued that ""AI-enabled capabilities could be used to threaten critical infrastructure, amplify disinformation campaigns, and wage war""18:1, and that ""global stability and nuclear deterrence could be undermined"".18:11 In 2014, former Secretary of DefenseChuck Hagelposited the ""Third Offset Strategy"" that rapid advances in artificial intelligence will define the next generation of warfare.19According to data science and analytics firm Govini, the U.S.Department of DefenseDoD increased investment in artificial intelligence, big data and cloud computing from 5.6 billion in 2011 to 7.4 billion in 2016.20However, the civilianNSFbudget for AI saw no increase in 2017.12Japan Timesreported in 2018 that the United States private investment is around 70 billion per year.21The November 2019 'Interim Report' of the United States' National Security Commission on Artificial Intelligence confirmed that AI is critical to US technological military superiority.18 The U.S. has many military AI combat programs, such as theSea Hunterautonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port.22From 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems.23On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand thekill-chainprocess. However, a major concern is how the report will be implemented.24 The Joint Artificial Intelligence Center JAIC pronounced ""jake""25is an American organization on exploring the usage of AI particularlyedge computing,Network of Networks, and AI-enhanced communication, for use in actual combat.26272829It is a subdivision of theUnited States Armed Forcesand was created in June 2018. The organization's stated objective is to ""transform theUS Department of Defenseby accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems; then, ensure the combat Systems and Components have real-time access to ever-improving libraries of data sets and tools.""27 In 2023 Microsoft pitched the DoD to useDALL-Emodels to train itsbattlefield management system.30OpenAI, thedeveloperof DALL-E, removed the blanket ban on military and warfare use from its usage policies in January 2024.31 Project Maven is aPentagonproject involving using machine learning and engineering talent to distinguish people and objects in drone videos,32apparently giving the government real-time battlefield command and control, and the ability to track, tag and spy on targets without human involvement. Initially the effort was led byRobert O. Workwho was concerned about China's military use of the emerging technology.33Reportedly, Pentagon development stops short of acting as an AI weapons system capable of firing on self-designated targets.34The project was established in a memo by theU.S. Deputy Secretary of Defenseon 26 April 2017.35Also known as theAlgorithmic Warfare Cross Functional Team,36it is, according to Lt. Gen. of theUnited States Air ForceJack Shanahan in November 2017, a project ""designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the Defense Department"".37Its chief,U.S. Marine CorpsCol. Drew Cukor, said: ""People and computers will work symbiotically to increase the ability of weapon systems to detect objects.""38Project Maven has been noted by allies, such as Australia'sIan Langford, for the ability to identify adversaries by harvesting data from sensors onUAVsand satellite.39At the secondDefense OneTech Summit in July 2017, Cukor also said that the investment in a ""deliberate workflow process"" was funded by the Department of Defense through its ""rapid acquisition authorities"" for about ""the next 36 months"".40 The U.S.Department of Defenseis partnering with Ukraine on ""Project Artemis"" to develop advanced drones that can withstand electronic warfare, blending Ukrainian simplicity and adaptability with American precision. Due to theRussia-Ukraine war, Ukraine has emerged as a leader in drone production and warfare, creating cost-effective systems that challenge traditional approaches. Countries like Turkey, China, and Iran are also producing affordable drones, reducing America's monopoly and reshaping warfare dynamics. U.S. efforts are focused on integrating AI,drone swarmtechnology, and hybrid drone systems to maintain military dominance. The democratization of drone technology raises issues, such as autonomous decision-making, counter-drone defenses, and dual-use concerns, that challenge ethical and security norms.41 China is pursuing a strategic policy ofmilitary-civil fusionon AI for globaltechnological supremacy.1842According to a February 2019 report by Gregory C. Allen of theCenter for a New American Security,China'sleadership includingparamount leaderXi Jinping believes that being at the forefront in AI technology is critical to the future of global military and economic power competition.7Chinese military officials have said that their goal is to incorporate commercial AI technology to ""narrow the gap between the Chinese military and global advanced powers.""7The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such asBaidupassing a notable Chinese-language speech recognition capability benchmark in 2015.43As of 2017, Beijing's roadmap aims to create a 150 billion AI industry by 2030.12Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies.44An October 2021 report by theCenter for Security and Emerging Technologyfound that ""Most of the Chinese military's AI equipment suppliers are not state-owned defense enterprises, but private Chinese tech companies founded after 2010.""45The report estimated that Chinese military spending on AI exceeded 1.6 billion each year.45TheJapan Timesreported in 2018 that annual private Chinese investment in AI is under 7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017; the Chinese filed for nearly five times as many AI patents as did Americans.21 China published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of theU. N. Security Councilto broach the issue.46In 2018, Xi called for greater international cooperation in basic AI research.47Chinese officials have expressed concern that AI such as drones could lead to accidental war, especially in the absence of international norms.48In 2019, formerUnited States Secretary of DefenseMark Esperlashed out at China for selling drones capable of taking life with no human oversight.49 The focus on ""intelligentized AI warfare"", pursued by China, suggests a comprehensive integration of AI across all domains land, sea, air, space, and cyber for autonomous attack, defence and cognitive warfare.50The intelligentized strategy is distinct from traditional warfare, which focuses on network-centric operations, and instead sees AI as a force multiplier that enhances decision-making, command structures, and autonomous capabilities. Unlike traditional warfare, intelligentization leverages AI to create a cognitive advantageallowing it to process battlefield information better. AI-assisted command-and-control C2 systems, predictive analytics, and real-time data fusion, enabling accelerated human-AI hybrid decision-making. Autonomous systems, including drone swarms, AI-powered cyber warfare, play a crucial role in this strategy. China is reported to be currently developing wingman drones, robotic ground forces, and optimised logistics to enhance combat effectiveness.51TheChinese army PLA also emphasisescognitive warfareusing AI-driven psychological operations, social media manipulation, and predictive behavioural analysis to influence adversaries and the importance of dynamic responses where AI enhances hacking capabilities, automated SIGINT Signals Intelligence and adaptive tactics. However, despite this focus, some analysts believe China could be struggling to fully realise AI capability within the military environment: a ""comprehensive review of dozens of Chinese-language journal articles about AI and warfare reveals that Chinese defense experts claim that Beijing is facing several technological challenges that may hinder its ability to capitalize on the advantages provided by military AI""52 A task force for the Strategic Implementation of AI for National Security and Defence was established in February 2018 by theMinistry of Defense's Department of Defence Production.53The process of getting the military ready for AI use was started by theMoDin 2019.54TheCentre for Artificial Intelligence and Roboticswas approved to develop AI solutions to improveintelligence collectionand analysis capabilities.55In 2021, theIndian Army, with assistance from theNational Security Council, began operating the Quantum Lab and Artificial Intelligence Center at theMilitary College of Telecommunication Engineering. With an emphasis on robotics and artificial intelligence,Defence Research and Development OrganisationandIndian Institute of Scienceestablished the Joint Advanced Technology Programme-Center of Excellence.5657In 2022, theIndian Navycreated an AI Core group and set up a Center of Excellence for AI andBig Data analysisatINS Valsura.5859Indian Army incubated Artificial Intelligence Offensive Drone Operations Project.6061During Exercise Dakshin Shakti 2021, the Indian Army integrated AI into itsintelligence,surveillance, andreconnaissancearchitecture.62 In 2022, the Indian government established theDefence Artificial Intelligence Counciland theDefence AI Project Agency,6364and it also published a list of 75 defense-related AI priority projects.6566MoDearmarked1,000croreannually till 2026 for capacity building, infrastructure setup, data preparation, and Al project implementation.67The Indian Army, the Indian Navy and theIndian Air Forceset aside 100 crore annually for the development of AI-specific applications.68The military is already deploying some AI-enabled projects and equipment.6970At Air Force Station Rajokri, theIAFCentre of Excellence for Artificial Intelligence was established in 2022 as part of the Unit for Digitization, Automation, Artificial Intelligence, and Application Networking UDAAN.71Swarm drone systemswere introduced by theMechanised Infantry Regimentfor offensive operations close to theLine of Actual Control.72 For offensive operations, the military began acquiring AI-enabledUAVsandswarm drones.737475Bharat Electronicsdeveloped AI-enabled audio transcription and analysis software for batlefield communication. Using AI during transport operations, the Indian Army's Research  Development branch patented driver tiredness monitoring system.7677As part of initial investment, theIndian Armed Forcesis investing about 50 million 47.2 million yearly on AI, according to Delhi Policy Group.78Forhigh altitudelogisticsat forward outposts,military robotsare deployed.7980Army is developing autonomous combat vehicles, robotic surveillance platforms, and Manned-Unmanned Teaming MUM-T solutions as part of the Defence AI roadmap.81MCTEis working with theMinistry of Electronics and Information Technologyand, Society for Applied Microwave Electronics Engineering  Research, on AI and military-gradechipset.8283Phase III of AI-enabled space-based surveillance has been authorized.8485 DRDOChairman and Secretary of the Department of Defense Research  Development Samir V. Kamat said the agency started concentrating on the potential use of AI in the development of military systems and subsystems.86The Indian government intends to leverage the private sector's sizable AI workforce anddual-use technologiesfor defense by 2026.87In order to conduct research on autonomous platforms, improved surveillance,predictive maintenance, andintelligent decision support system, the Indian Army AI Incubation Center was established.88Indian Navy launchedINS Suratwith AI capabilities.8990 Russian GeneralViktor Bondarev, commander-in-chief of the Russian air force, stated that as early as February 2017, Russia was working on AI-guided missiles that could decide to switch targets mid-flight.91TheMilitary-Industrial Commission of Russiahas approved plans to derive 30 percent of Russia's combat power from remote controlled and AI-enabled robotic platforms by 2030.92Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017.93In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that ""there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact"", and that it is inevitable that ""swarms of drones"" will one day fly over combat zones.94Russia has been testing several autonomous and semi-autonomous combat systems, such asKalashnikov's ""neural net"" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention.22 In September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian PresidentVladimir Putinstated ""Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world"". Putin also said it would be better to prevent any single actor achieving a monopoly, but that if Russia became the leader in AI, they would share their ""technology with the rest of the world, like we are doing now with atomic and nuclear technology"".959697 Russia is establishing a number of organizations devoted to the development of military AI. In March 2018, the Russian government released a 10-point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state-backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives.98In addition, Russia recently created a defense research organization, roughly equivalent to DARPA, dedicated to autonomy and robotics called the Foundation for Advanced Studies, and initiated an annual conference on ""Robotization of the Armed Forces of the Russian Federation.""99100 The Russian military has been researching a number of AI applications, with a heavy emphasis on semiautonomous and autonomous vehicles. In an official statement on November 1, 2017, Viktor Bondarev, chairman of the Federation Council's Defense and Security Committee, stated that ""artificial intelligence will be able to replace a soldier on the battlefield and a pilot in an aircraft cockpit"" and later noted that ""the day is nearing when vehicles will get artificial intelligence.""101Bondarev made these remarks in close proximity to the successful test of Nerehta, an crewless Russian ground vehicle that reportedly ""outperformed existing crewed combat vehicles."" Russia plans to use Nerehta as a research and development platform for AI and may one day deploy the system in combat, intelligence gathering, or logistics roles.102Russia has also reportedly built a combat module for crewless ground vehicles that is capable of autonomous target identificationand, potentially, target engagementand plans to develop a suite of AI-enabled autonomous systems.103104100 In addition, the Russian military plans to incorporate AI into crewless aerial, naval, and undersea vehicles and is currently developing swarming capabilities.99It is also exploring innovative uses of AI for remote sensing and electronic warfare, including adaptive frequency hopping, waveforms, and countermeasures.105106Russia has also made extensive use of AI technologies for domestic propaganda and surveillance, as well as for information operations directed against the United States and U.S. allies.107108100 The Russian government has strongly rejected any ban onlethal autonomous weaponsystems, suggesting that such an international ban could be ignored.109110 The Russian invasion of Ukraine and the ensuing Russia-Ukraine war has seen seen significant use of AI by both sides and has also been characterised as a drone war.111Advances in AI-powered GPS-denied navigation and drone swarming techniques are significantly improving operational capabilities for Ukraine. Fully realised drone swarms, where multiple drones coordinate and make decisions autonomously, are still in the early stages of experimentation but Ukraine is exploring and implementing these techniques in a real conflict situation.112The Defense Intelligence of Ukraine DIU has been at the forefront of utilizing drones with some elements of autonomy for conducting long-range strikes into Russian territory. Domestic drone production has significantly expanded, with approximately 2 million drones produced in 2024, 96.2 of which were domestically manufactured.113 Rather than replacing human involvement, AI is primarily serving to augment existing capabilities, enhancing the speed, accuracy, and overall efficiency of numerous military functions. Perhaps the most important way in which AI has been used by Ukraine is in intelligence, surveillance, and reconnaissance ISR capabilities. The Ukrainian military uses Palantirs MetaConstellation software to monitor the movement of Russian troops and supplies highlighting the blurring of boundaries between state military and commercial AI use. It aggregates data from various commercial civilian providers of satellite imagery Ukraine also uses its own Delta system which aggregates real time data from drone imagery, satellite photos, acoustic signals, and text to construct an operational picture for military commanders. AI is used to prioritise incoming threats, potential targets and resource constraints.114 AI is also being used to process intercepted communications from Russian soldiers, to process, select, and output militarily useful information from these intercepted calls. Israel makes extensive use of AI for military applications specially during theGaza war. The main AI systems used fortarget identificationare the Gospel and Lavender. Lavender developed by the Unit 8200 identifies and creates a database of individuals mostly low-ranking militants of Hamas and the Palestinian Islamic Jihad and has a 90 accuracy rate and a database of tens of thousands. The Gospel in comparisons recommended buildings and structures rather than individuals. The acceptable collateral damage and the type of weapon used to eliminate the target is decided by IDF members and could track militants even when at home.115 Israel'sHarpyanti-radar ""fire and forget"" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria.116The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions.117These robotic vehicles are used in border defense. In 2015, the UK government opposed a ban on lethal autonomous weapons, stating that ""international humanitarian law already provides sufficient regulation for this area"", but that all weapons employed by UK armed forces would be ""under human oversight and control"".118 The South KoreanSuper aEgis IImachine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, ""Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability"", and they want to ""get to a place where our software can discern whether a target is friend, foe, civilian or military"".119 The European Parliament holds the position that humans must have oversight and decision-making power over lethal autonomous weapons.120However, it is up to each member state of the European Union to determine their stance on the use of autonomous weapons and the mixed stances of the member states is perhaps the greatest hindrance to the European Union's ability to develop autonomous weapons. Some members such as France, Germany, Italy, and Sweden are developing lethal autonomous weapons. Some members remain undecided about the use of autonomous military weapons and Austria has even called to ban the use of such weapons.121 Some EU member states have developed and are developing automated weapons. Germany has developed anactive protection system, the Active Defense System, that can respond to a threat with complete autonomy in less than a millisecond.121122Italy plans to incorporate autonomous weapons systems into its future military plans.121 The international regulation of autonomous weapons is an emerging issue for international law.123AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.12As early as 2007, scholars such as AI professorNoel Sharkeyhave warned of ""an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions"".124125 Miles Brundage of theUniversity of Oxfordhas argued an AI arms race might be somewhat mitigated through diplomacy: ""We saw in the various historical arms races that collaboration and dialog can pay dividends"".126Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons;127128however, at a November 2017 session of the UNConvention on Certain Conventional WeaponsCCW, diplomats could not agree even on how to define such weapons.129The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect.130As of 2019, 26 heads of state and 21 Nobel Peace Prize laureates have backed a ban on autonomous weapons.131However, as of 2022, most major powers continue to oppose a ban on autonomous weapons.132 Many experts believe attempts to completely ban killer robots are likely to fail,133in part because detecting treaty violations would be extremely difficult.134135A 2017 report fromHarvard'sBelfer Centerpredicts that AI has the potential to be as transformative as nuclear weapons.126136137The report further argues that ""Preventing expanded military use of AI is likely impossible"" and that ""the more modest goal of safe and effective technology management must be pursued"", such as banning the attaching of an AIdead man's switchto a nuclear arsenal.137 A 2015 open letter by theFuture of Life Institutecalling for the prohibition of lethal autonomous weapons systems has been signed by over 26,000 citizens, including physicistStephen Hawking,TeslamagnateElon Musk,Apple'sSteve WozniakandTwitterco-founderJack Dorsey, and over 4,600 artificial intelligence researchers, includingStuart Russell,Bart SelmanandFrancesca Rossi.138129The Future of Life Institute has also released two fictional films,Slaughterbots2017 andSlaughterbots - if human: kill2021, which portray threats of autonomous weapons and promote a ban, both of which went viral. ProfessorNoel Sharkeyof theUniversity of Sheffieldargues that autonomous weapons will inevitably fall into the hands of terrorist groups such as theIslamic State.139 Many Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China's market.43Furthermore, some researchers, such asDeepMindCEODemis Hassabis, are ideologically opposed to contributing to military work.140 For example, in June 2018, company sources atGooglesaid that top executiveDiane Greenetold staff that the company would not follow-up Project Maven after the current contract expired in March 2019.32 TITLE: Artificial intelligence arms race - Wikipedia",2025-05-06 13:42:47
https://en.wikipedia.org/wiki/Artificial_intelligence_and_copyright,Artificial intelligence and copyright - Wikipedia,"Artificial intelligence and copyright Contents Copyright status of AI-generated works Training AI with copyrighted data Copyright infringing AI outputs Litigation References External links United States United Kingdom China United States EU UK India In the 2020s, therapid advancementofdeep learning-basedgenerative artificial intelligencemodels raised questions about whethercopyright infringementoccurs when such are trained or used. This includestext-to-image modelssuch asStable Diffusionandlarge language modelssuch asChatGPT. As of 2023, there were several pending U.S. lawsuits challenging the use of copyrighted data to train AI models, with defendants arguing that this falls underfair use.1 Popular deep learning models are trained on mass amounts of mediascrapedfrom the Internet, often utilizing copyrighted material.2When assembling training data, the sourcing of copyrighted works may infringe on thecopyright holder's exclusive right to control reproduction, unless covered by exceptions in relevant copyright laws. Additionally, using a model's outputs might violate copyright, and the model creator could be accused ofvicarious liabilityand held responsible for that copyright infringement. Since most legal jurisdictions only grant copyright to original works of authorship by human authors, the definition of ""originality"" is central to the copyright status of AI-generated works.3 In the U.S., theCopyright Actprotects ""original works of authorship"".4TheU.S. Copyright Officehas interpreted this as being limited to works ""created by a human being"",4declining to grant copyright to works generated without human intervention.5Some legal professionals have suggested thatNaruto v. Slater2018, in which theU.S. 9th Circuit Court of Appealsheld thatnon-humanscannot be copyright holders ofartistic works, could be a potential precedent in copyright litigation over works created by generative AI.6Some have suggested that certain AI generations might be copyrightable in the U.S. and similar jurisdictions if it can be shown that the human who ran the AI program exercised sufficient originality in selecting the inputs to the AI or editing the AI's output.54 Proponents of this view suggest that an AI model may be viewed as merely a tool akin to a pen or a camera used by its human operator to express their creative vision.47For example, proponents argue that if the standard of originality can be satisfied by an artist clicking the shutter button on a camera, then perhaps artists using generative AI should get similar deference, especially if they go through multiple rounds of revision to refine their prompts to the AI.8Other proponents argue that the Copyright Office is not taking a technology neutral approach to the use of AI oralgorithmictools. For other creative expressions music, photography, writing the test is effectively whether there isde minimis,or limited human creativity. For works using AI tools, the Copyright Office has made the test a different one i.e. whether there is no more thande minimistechnological involvement.9 This difference in approach can be seen in the recent decision in respect of a registration claim by Jason Matthew Allen for his workTh√©√¢tre D'op√©ra Spatialcreated using Midjourney and an upscaling tool. The Copyright Office stated: The Board finds that the Work contains more than a de minimis amount of content generated by artificial intelligence ""AI"", and this content must therefore be disclaimed in an application for registration. Because Mr. Allen is unwilling to disclaim the AI-generated material, the Work cannot be registered as submitted.10 As AI is increasingly used to generate literature, music, and other forms of art, the U.S. Copyright Office has released new guidance emphasizing whether works, including materials generated by artificial intelligence, exhibit a 'mechanical reproduction' nature or are the 'manifestation of the author's own creative conception'.11The U.S. Copyright Office published a Rule in March 2023 on a range of issues related to the use of AI, where they stated: ...because the Office receives roughly half a million applications for registration each year, it sees new trends in registration activity that may require modifying or expanding the information required to be disclosed on an application. One such recent development is the use of sophisticated artificial intelligence ""AI"" technologies capable of producing expressive material. These technologies ""train"" on vast quantities of preexisting human-authored works and use inferences from that training to generate new content. Some systems operate in response to a user's textual instruction, called a ""prompt."" The resulting output may be textual, visual, or audio, and is determined by the AI based on its design and the material it has been trained on. These technologies, often described as ""generative AI,"" raise questions about whether the material they produce is protected by copyright, whether works consisting of both human-authored and AI-generated material may be registered, and what information should be provided to the Office by applicants seeking to register them.12 The Copyright Office further clarified in a January 2025 that AI-assisted works which the creative expression of the human remains evident in the work can be copyrighted, which can include creative adaption of prompts for AI generators or usage of AI to assist in creation process of a work such asfilmmaking.13Works ""where the expressive elements are determine by a machine"" still remain uncopyrightable.14Following this guidance, the Copyright Office registered ""A Single Piece of American Cheese"", the first visual artwork composed solely of AI generated outputs as a composite work in January 2025.15The basis for the copyright involved arguing that human-driven selection, arrangement, and coordination involved in the creative process on a single work constituted sufficient human authorship to merit the copyright. Both the federal and circuit courts in the District of Columbia have upheld the Copyright Office's refusal to register copyrights for works generated solely by machines, establishing that machine ownership would conflict with heritable property rights as establish by the Copyright Act of 1975.16 TheU.S. Patent and Trademark OfficeUSPTO similarly codified restrictions on thepatentabilityof patents credits solely to AI authors in February 2024, following an August 2023 ruling in the caseThaler v. Perlmutter. In this case, the Patent Office denied grant to patents created by Stephen Thaler's AI program,DABUSdue to the lack of a ""natural person"" on the patents' authorship. TheU.S. Court of Appealsfor the Federal Circuit upheld this decision.1718In the subsequent rule-making, the USPTO allows for human inventors to incorporate the output of artificial intelligence, as long as this method is appropriately documented in the patent application.19However, it may become virtually impossible as when the inner workings and the use of AI in inventive transactions are not adequately understood or are largely unknown.18 RepresentativeAdam Schiffproposed theGenerative AI Copyright Disclosure Actin April 2024. If passed, the bill would require AI companies to submit copyrighted works to theRegister of Copyrightsbefore releasing new generative AI systems. These companies would have to file these documents 30 days before publicly showing their AI tools.20 Other jurisdictions include explicit statutory language related to computer-generated works, including the United Kingdom'sCopyright, Designs and Patents Act 1988, which states: In the case of a literary, dramatic, musical or artistic work which is computer-generated, the author shall be taken to be the person by whom the arrangements necessary for the creation of the work are undertaken.7 However, the computer generated work law under UK law relates to autonomous creations by computer programs. Individuals using AI tools will usually be the authors of the works assuming they meet the minimum requirements for copyright work. The language used for computer generated work relates, in respect of AI, to the ability of the human programmers to have copyright in the autonomous productions of the AI tools i.e. where there is no direct human input: In so far as each composite frame is a computer generated work then the arrangements necessary for the creation of the work were undertaken by Mr Jones because he devised the appearance of the various elements of the game and the rules and logic by which each frame is generated and he wrote the relevant computer program. In these circumstances I am satisfied that Mr Jones is the person by whom the arrangements necessary for the creation of the works were undertaken and therefore is deemed to be the author by virtue of s.9321 The UK government has consulted on the use of generative tools and AI in respect of intellectual property leading to a proposed specialist Code of Practice:22""to provide guidance to support AI firms to access copyrighted work as an input to their models, whilst ensuring there are protections on generated output to support right holders of copyrighted work"".23The U.S. Copyright Office recentlywhen?published anotice of inquiryand request for comments following its 2023 Registration Guidance.24 On November 27, 2023, theBeijingInternet Court issued a decision recognizing copyright in AI-generated images in a litigation.25 As noted by a lawyer and AI art creator, the challenge for intellectual property regulators, legislators and the courts is how to protect human creativity in a technologically neutral fashion whilst considering the risks of automated AI factories. AI tools have the ability to autonomously create a range of material that is potentially subject to copyright music, blogs, poetry, images, and technical papers or other intellectual property rights patents and design rights.9 Deep learning modelssourcelarge data sets from the Internet such as publicly available images and the text of web pages. The text and images are then converted intonumeric formatsthe AI can analyze. A deep learning model identifies patterns linking the encoded text and image data and learns which text concepts correspond to elements in images. Through repetitive testing, the model refines its accuracy by matching images to text descriptions. The trained model undergoes validation to evaluate its skill in generating or manipulating new images using only the text prompts provided after the training process.26When assembling thesetraining datasetsinvolves making copies of copyrighted works, this has raised the question of whether this process infringes the copyright holder's exclusive right to make reproductions of their works, or if it falls usefair useallowances.2728 U.S. machine learning developers have traditionally believed this to be allowable under fair use because using copyrighted work istransformative, and limited.29The situation has been compared toGoogle Books's scanning of copyrighted books inAuthors Guild, Inc. v. Google, Inc., which was ultimately found to be fair use, because the scanned content was not made publicly available, and the use was non-expressive.30 Timothy B. Lee, inArs Technica, argues that if theplaintiffssucceed, this may shift the balance of power in favour of large corporations such as Google, Microsoft, and Meta which can afford to license large amounts of training data from copyright holders and leverage their proprietary datasets of user-generated data.31IPscholars Bryan Casey andMark Lemleyargue in theTexas Law Reviewthat datasets are so large that ""there is no plausible option simply to license all of the data.... So allowing any generative training copyright claim is tantamount to saying, not that copyright owners will get paid, but that the use won't be permitted at all.""32Other scholars disagree; some predict a similar outcome to the U.S.music licensingprocedures.29 One of the earliest case to challenge the nature of fair use for training AI was a lawsuit thatThomson Reutersbrought against Ross Intelligence first filed in 2020. Thomson Reuters argued that Ross Intelligence had used theirWestlawheadnotes, brief summaries of court decisions, to train their AI engine designed to compete with Westlaw. While Thomson Reuters' claims were initially denied by judgeStephanos Bibasof theThird Circuiton the basis that headnotes may not have been copyrightable, Bibas reevaluated his decision in February 2025 and issued a ruling favoring Thomson Reuters, in that headnotes are copyrightable, and that Ross Intelligence, which had since closed down in 2021, had inappropriately used the material. In the case of Ross's AI, the engine was not generative, and produced output that was composed of pieces of Westlaw's material, which aided in Thomson Reuter's claims of reuse, so how the case may apply to other generative AI like OpenAI is not clear.33 In a consolidated case brought by several authors against Meta and OpenAI, federal district judgeVince Chhabriaexpressed doubt that the use of unlicensed copyrighted material for training AI would fall under fair use. He stated during court hearings to Meta's lawyers that ""You have companies using copyright-protected material to create a product that is capable of producing an infinite number of competing products. You are dramatically changing, you might even say obliterating, the market for that person's work, and you're saying that you don't even have to pay a license to that person. I just don't understand how that can be fair use.""34 In the EU, such TDM exceptions form part of the 2019Directive on Copyright in the Digital Single Market.35They are specifically referred to in the EU'sAI Actwhich came into force in 2024, which ""is widely seen as a clear indication of the EU legislators intention that the exception covers AI data collection"", a view that was also endorsed in a 2024 German court decision.36Unlike the TDM exception for scientific research, the more general exception covering commercial AI only applies if the copyright holder has not opted out.36In order to facilitate the opt-out to the TDM exception, the EU's AI Act of 2024 requires providers of ""general-purpose"" AI models to implement a policy to comply with EU law including the TDM exception opt-out and to publish a detailed summary of training content according to a template provided by the AI Office. These provisions will come into force in August 2025, with further clarification on exactly what will be required to providers of general-purpose AI models expected to come from a Code of Practice to be released in advance of this.37 Unlike the EU, the United Kingdom prohibits data mining for commercial purposes but has proposed this should be changed to support the development of AI: ""For text and data mining, we plan to introduce a new copyright and database exception which allows TDM for any purpose. Rights holders will still have safeguards to protect their content, including a requirement for lawful access.""38 Indian copyright law provides fair use exceptions for scientific research, but lacks specific provisions for commercial AI training models. Unlike the EU and UK, India has not established text and data mining TDM provisions that explicitly address commercial AI systems. This regulatory uncertainty became apparent in 2024 whenAsian News InternationalANI sued OpenAI for using its content to train AI models without authorization. While OpenAI offered an opt-out policy that ANI used in October 2024 to block AIscrapers, ANI claimed this measure was ineffective since their content remained available throughcontent syndication. The case also highlighted jurisdictional challenges, as OpenAI argued it was not subject to Indian law because its servers and training operations were located outside the country.3940 In some cases, deep learning models may replicate items in their training set when generating output. This behaviour is generally considered an undesiredoverfittingof a model by AI developers, and has in previous generations of AI been considered a manageable problem.42Memorizationis the emergent phenomenon of LLMs to repeat long strings of training data, and it is no longer related to overfitting.43Evaluations of controlled LLM output measure the amount memorized from training data focused onGPT-2-series models as variously over 1 for exact duplicates44or up to about 7.45This is potentially a security risk and a copyright risk, for both users and providers.46As of August 2023update, major consumer LLMs have attempted to mitigate these problems, but researchers have still been able to prompt leakage of copyrighted material.47 Under U.S. law, to prove that an AI output infringes a copyright, a plaintiff must show the copyrighted work was ""actually copied"", meaning that the AI generates output which is ""substantially similar"" to their work, and that the AI had access to their work.4 In the course of learning to statistically model the data on which they are trained, deep generative AI models may learn to imitate the distinct style of particular authors in the training set. Sincefictional characters enjoy some copyright protectionin the U.S. and other jurisdictions, an AI may also produce infringing content in the form of novel works which incorporate fictional characters.441 A generative image model such as Stable Diffusion is able to model the stylistic characteristics of an artist likePablo Picassoincluding his particular brush strokes, use of colour, perspective, and so on, and a user can engineer a prompt such as ""an astronaut riding a horse, by Picasso"" to cause the model to generate a novel image applying the artist's style to an arbitrary subject. However, an artist's overall style is generally not subject to copyright protection.4Additional questions related to the copyrightability of style and the output of AI models was raised in March 2025, following an update to ChatGPT's model that was able to produce images strongly resembling the work ofStudio Ghibli's artistHayao Miyazaki. While users initially used it to make ""Ghiblification"" of popular meme images, further users were found to be distasteful in light of Miyazaki's negative stance on AI, and ChatGPT placed limits on the ability for users to make images in the style of living artists.4849 TITLE: Artificial intelligence and copyright - Wikipedia",2025-05-06 13:42:49
